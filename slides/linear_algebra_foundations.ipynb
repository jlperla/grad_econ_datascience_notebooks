{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Foundations of Numerical Linear Algebra\n",
        "\n",
        "Graduate Quantitative Economics and Datascience\n",
        "\n",
        "Jesse Perla (University of British Columbia)\n",
        "\n",
        "## Going Beyond “`reg y x, robust`”\n",
        "\n",
        "-   Data science, econometrics, and macroeconomics are built on linear\n",
        "    algebra.\n",
        "-   Numerical linear algebra has all sorts of pitfalls, which become\n",
        "    more critical as we scale up to larger problems.\n",
        "-   Speed differences in choosing better algorithms can be orders of\n",
        "    magnitude.\n",
        "-   Crucial to know what goes on under-the-hood in Stata/R/python\n",
        "    packages for applied work, even if you don’t implement it yourself.\n",
        "\n",
        "## Extra Materials\n",
        "\n",
        "-   Material related to: [QuantEcon\n",
        "    Python](https://python.quantecon.org/linear_algebra.html),\n",
        "    [QuantEcon Data\n",
        "    Science](https://datascience.quantecon.org/scientific/applied_linalg.html),\n",
        "    [Intro Quantitative Economics with\n",
        "    Python](https://intro.quantecon.org/)\n",
        "-   **Self-study and Optional Materials:**\n",
        "    -   [Basics of linear algebra, matrices, norms, and linear\n",
        "        independence](https://python.quantecon.org/linear_algebra.html)\n",
        "    -   [Numerical\n",
        "        optimization](https://datascience.quantecon.org/scientific/optimization.html)\n",
        "    -   [Systems of\n",
        "        Equations](https://python.quantecon.org/linear_algebra.html#solving-systems-of-equations)\n",
        "    -   [Eigenvectors and\n",
        "        eigenvalues](https://python.quantecon.org/linear_algebra.html#eigenvalues-and-eigenvectors)\n",
        "    -   [Downloading and manipulating data in\n",
        "        Python](https://intro.quantecon.org/long_run_growth.html) and\n",
        "        [here](https://intro.quantecon.org/business_cycle.html)\n",
        "    -   [Introductory material on linear\n",
        "        algebra](https://intro.quantecon.org/linear_equations.html) and\n",
        "        [more](https://datascience.quantecon.org/scientific/applied_linalg.html)\n",
        "    -   [Matrix decompositions and other\n",
        "        topics](https://python.quantecon.org/linear_algebra.html#further-topics)\n",
        "\n",
        "## Packages\n",
        "\n",
        "This section uses the following packages:"
      ],
      "id": "c7b89fe2-600f-4f37-a4ce-54ba44bba136"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from numpy.linalg import cond, matrix_rank, norm\n",
        "from scipy.linalg import inv, solve, det, eig, lu, eigvals\n",
        "from scipy.linalg import solve_triangular, eigvalsh, cholesky"
      ],
      "id": "6fb5cd81"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Computational Complexity\n",
        "\n",
        "**Big-O Notation**\n",
        "\n",
        "For a function $f(N)$ and a positive constant $C$, we say $f(N)$ is\n",
        "$O(g(N))$, if there exist positive constants $C$ and $N_0$ such that:\n",
        "\n",
        "$$\n",
        "0 \\leq f(N) \\leq C \\cdot g(N) \\quad \\text{for all } N \\geq N_0\n",
        "$$\n",
        "\n",
        "-   Often crucial to know how problems scale asymptotically (as\n",
        "    $N\\to\\infty$)\n",
        "-   Caution! This is only an asymptotic limit, and can be misleading for\n",
        "    small $N$\n",
        "    -   $f_1(N) = N^3 + N$ is $O(N^3)$\n",
        "    -   $f_2(N) = 1000 N^2 + 3 N$ is $O(N^2)$\n",
        "    -   For roughly $N>1000$ use $f_2$ algorithm, otherwise $f_1$\n",
        "\n",
        "## Examples of Computational Complexity\n",
        "\n",
        "-   Simple examples:\n",
        "    -   $x \\cdot y = \\sum_{n=1}^N x_n y_n$ is $O(N)$ since it requires\n",
        "        $N$ multiplications and additions\n",
        "    -   $A x$ for $A\\in\\mathbb{R}^{N\\times N},x\\in\\mathbb{R}^N$ is\n",
        "        $O(N^2)$ since it requires $N$ dot products, each $O(N)$\n",
        "\n",
        "## Numerical Precision\n",
        "\n",
        "**Machine Epsilon**\n",
        "\n",
        "For a given datatype, $\\epsilon$ is defined as\n",
        "$\\epsilon = \\min_{\\delta > 0} \\left\\{ \\delta : 1 + \\delta > 1 \\right\\}$\n",
        "\n",
        "-   Computers have finite precision. 64-bit typical, but 32-bit on GPUs"
      ],
      "id": "89afafbb-6437-4ee0-a7bd-77bf9a029c25"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "machine epsilon for float64 = 2.220446049250313e-16\n",
            "1 + eps/2 == 1? True\n",
            "machine epsilon for float32 = 1.1920928955078125e-07"
          ]
        }
      ],
      "source": [
        "print(f\"machine epsilon for float64 = {np.finfo(float).eps}\")\n",
        "print(f\"1 + eps/2 == 1? {1.0 + 1.1e-16 == 1.0}\")\n",
        "print(f\"machine epsilon for float32 = {np.finfo(np.float32).eps}\")"
      ],
      "id": "d87c57fe"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basic Linear Algebra\n",
        "\n",
        "## Norms\n",
        "\n",
        "-   Common measure of size is the Euclidean norm, or $L^2$ norm for\n",
        "    $x \\in \\mathbb{R}^2$\n",
        "-   Complexity is $O(N)$, square $N$ times then $N$ additions $$\n",
        "    ||x||_2 = \\sqrt{\\sum_{n=1}^N x_n^2}\n",
        "    $$"
      ],
      "id": "8519cd2c-b204-4856-860c-4f6b5c3c14ac"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.7416573867739413\n",
            "3.7416573867739413\n",
            "3.7416573867739413\n",
            "||x||_2^2 = 14.0 = 14 = 14"
          ]
        }
      ],
      "source": [
        "x = np.array([1, 2, 3]) # Calculating different ways (in order of preference)\n",
        "print(np.sqrt(sum(xval**2 for xval in x))) # manual with comprehensions\n",
        "print(np.sqrt(np.sum(np.square(x)))) # broadcasts\n",
        "print(norm(x)) # built-in to numpy norm(x, ord=2) alternatively\n",
        "print(f\"||x||_2^2 = {norm(x)**2} = {x.T @ x} = {np.dot(x, x)}\")"
      ],
      "id": "92325e00"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solving Systems of Equations\n",
        "\n",
        "-   Solving $A x = b$ for $x$ is equivalent $A^{-1} A x = A^{-1} b$\n",
        "-   Then since $A^{-1}A = I$, and $I x = x$, we have $x = A^{-1} b$\n",
        "-   Careful since matrix algebra is not commutative!"
      ],
      "id": "49819400-3fec-4b5d-8e3b-23fda7718b78"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([-1.,  1.])"
            ]
          }
        }
      ],
      "source": [
        "A = np.array([[0, 2],\n",
        "              [3, 4]]) # or ((0, 2), (3, 4))\n",
        "b = np.array([2,1])  # Column vector\n",
        "x = solve(A, b)  # Solve Ax = b for x\n",
        "x"
      ],
      "id": "18ddd006"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using the Inverse Directly\n",
        "\n",
        "-   Can replace the `solve` with a calculation of an inverse\n",
        "-   But it can be slower or less accurate than solving the system\n",
        "    directly"
      ],
      "id": "3cb93354-c26a-47de-8c54-cb3b8404a111"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([-1.,  1.])"
            ]
          }
        }
      ],
      "source": [
        "A_inv = inv(A)\n",
        "A_inv @ b # i.e, A^{-1} * b"
      ],
      "id": "261ac952"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Combinations\n",
        "\n",
        "We can think of solving a system as finding the linear combination of\n",
        "columns of $A$ that equal $b$"
      ],
      "id": "e9b785a1-a60c-47ca-b898-1089e8ed0b14"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b = [2 1], b_star = [2. 1.]"
          ]
        }
      ],
      "source": [
        "b_star = x[0] * A[:, 0] + x[1] * A[:, 1] # using x solution\n",
        "print(f\"b = {b}, b_star = {b_star}\")"
      ],
      "id": "4445dd72"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Column Space and Rank\n",
        "\n",
        "-   The column space of a matrix represents all possible linear\n",
        "    combinations of its columns.\n",
        "-   It forms a basis for the space of solutions when solving systems of\n",
        "    linear equations represented by the matrix\n",
        "-   The rank of a matrix is the dimension of its column space"
      ],
      "id": "24e646aa-880f-422e-859d-7a7dedbfdc90"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "np.int64(2)"
            ]
          }
        }
      ],
      "source": [
        "A = np.array([[0, 2],\n",
        "              [3, 4]])\n",
        "matrix_rank(A)"
      ],
      "id": "9e7c0628"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hence, can solve $A x = b$ for any $b \\in \\mathbb{R}^2$ since the column\n",
        "space is the entire space $\\mathbb{R}^2$\n",
        "\n",
        "## Singular Matrices\n",
        "\n",
        "On the other hand, note"
      ],
      "id": "79c8958e-199b-490e-9017-89e3594829ac"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "np.int64(1)"
            ]
          }
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 4]])\n",
        "matrix_rank(A)"
      ],
      "id": "594bd358"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So we can only solve $A x = b$ for\n",
        "$b \\propto \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} \\propto \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$\n",
        "\n",
        "## Checking Singularity"
      ],
      "id": "a5f36945-f5d9-4c53-8707-75dd35416e9d"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "Matrix is singular (non-invertible)."
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 4]])\n",
        "# An (expensive) way to check if A is singular is if det(A) = 0\n",
        "print(det(A) == 0.0)\n",
        "print(matrix_rank(A) != A.shape[0]) # or check rank\n",
        "# Check before inverting or use exceptions\n",
        "try:\n",
        "    inv(A)\n",
        "    print(\"Matrix is not singular (invertible).\")\n",
        "except np.linalg.LinAlgError:\n",
        "    print(\"Matrix is singular (non-invertible).\")"
      ],
      "id": "0b6a07a3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Determinant is Not Scale Invariant\n",
        "\n",
        "-   Reminder: numerical precision in calculations makes it hard to\n",
        "    compare to zero\n",
        "-   The determinate is useful but depends on the scale of the matrix\n",
        "-   A more robust alternative is the condition number (more next\n",
        "    lecture)"
      ],
      "id": "b28e4b0d-2b30-4f54-804f-c007c3e693af"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "det(A)=-1e-08, det(K*A)=-100\n",
            "cond(A)=1e+09, cond(K*A)=1e+09,\n",
            "det(inv(A))=-1e+08, cond(inv(A))=1e+09"
          ]
        }
      ],
      "source": [
        "eps, K = 1e-8, 100000\n",
        "A = np.array([[1, 2],\n",
        "              [1 + eps, 2 + eps]]) # not 1 + eps, 2(1+eps)!\n",
        "print(f\"det(A)={det(A):.5g}, det(K*A)={det(K*A):.5g}\")\n",
        "print(f\"cond(A)={cond(A):.5g}, cond(K*A)={cond(K*A):.5g},\")\n",
        "print(f\"det(inv(A))={det(inv(A)):.5g}, cond(inv(A))={cond(inv(A)):.5g}\")"
      ],
      "id": "71638e9e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpreting Condition Numbers\n",
        "\n",
        "-   The condition number of the matrix $A$ is\n",
        "    $\\kappa(A) = ||A|| \\cdot ||A^{-1}||$, which can be shown in terms of\n",
        "    ratio of the largest and smallest eigenvalues\n",
        "    -   $\\kappa(A) = \\left| \\frac{\\lambda_{\\text{max}}}{\\lambda_{\\text{min}}} \\right|$\n",
        "        for $\\lambda$ the eigenvalues of $A$. More soon!\n",
        "-   Crude intuition: for machine epsilon $\\epsilon_{\\text{mach}}$ when\n",
        "    calculating some $x$\n",
        "    -   The relative error, $||x - x_{\\text{approx}}||/||x||$ is roughly\n",
        "        $\\kappa(A) \\cdot \\epsilon_{\\text{mach}}$\n",
        "    -   Solving $A x = b$ when $\\epsilon_{\\text{mach}} = 1e^{-16}$ it\n",
        "        amplifies errors in $b$, etc.\n",
        "    -   if $\\kappa(A) \\approx 1e^{16}$ errors amplified so the scale of\n",
        "        100% relative error\n",
        "\n",
        "## Rules of Thumb\n",
        "\n",
        "-   Rule of thumb for standard floating points where\n",
        "    $\\epsilon_{\\text{mach}} = 1e^{-16}$:\n",
        "    -   $\\kappa(A) \\approx 1$ well-conditioned\n",
        "    -   $\\kappa(A) < 100$ fairly well-conditioned\n",
        "    -   $\\kappa(A) < 1e^{5}$ moderately ill-conditioned. Take care\n",
        "    -   $\\kappa(A) < 1e^{8}$ ill-conditioned and might introduce\n",
        "        significant errors, especially in algorithms which repeatedly\n",
        "        use the same calculations\n",
        "    -   $\\kappa(A) > 1e^{8}$ very ill-conditioned and likely to\n",
        "        introduce significant errors\n",
        "-   Choose solution algorithms based on “numerical stability” and\n",
        "    “conditioning” when worried\n",
        "-   Much more extreme with 32-bit floats such as when using GPUs.\n",
        "\n",
        "# Solving Linear Systems of Equations\n",
        "\n",
        "## Solving Systems with Multiple RHS\n",
        "\n",
        "-   Inverse is nice because you can reuse the $A^{-1}$ to solve\n",
        "    $A x = b$ for many $b$\n",
        "-   However, you can do this with `solve` as well\n",
        "-   Or can reuse LU factorizations (discussed next)"
      ],
      "id": "80030d97-559b-4a4f-a4b4-1cfb5c8ef57f"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.         -1.33333333]\n",
            " [ 1.          1.5       ]]\n",
            "Checking: A*[-1.  1.] = [2. 1.] = [2 1], column of B"
          ]
        }
      ],
      "source": [
        "A = np.array([[0, 2],\n",
        "              [3, 4]])\n",
        "B = np.array([[2, 3],\n",
        "              [1, 2]])  # [2,1] and [3,2] as columns\n",
        "# or: B = np.column_stack([np.array([2, 1]),np.array([3,2])])\n",
        "X = solve(A, B)  # Solve AX = B for X\n",
        "print(X)\n",
        "print(f\"Checking: A*{X[:,0]} = {A@X[:, 0]} = {B[:,0]}, column of B\")"
      ],
      "id": "9c256bfa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LU(P) Decompositions\n",
        "\n",
        "-   We can “factor” any square $A$ into $P A = L U$ for triangular $L$\n",
        "    and $U$. Invertible can have $A = L U$, called the LU decomposition.\n",
        "    “P” is for partial-pivoting\n",
        "-   Singular matrices may not have full-rank $L$ or $U$ matrices"
      ],
      "id": "b7200cac-8ac9-4548-a707-d4ea431c0bcb"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L*U =\n",
            "[[2. 4.]\n",
            " [1. 2.]]\n",
            "P*A =\n",
            "[[2. 4.]\n",
            " [1. 2.]]"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 4]])\n",
        "P, L, U = lu(A)\n",
        "print(f\"L*U =\\n{L @ U}\")\n",
        "print(f\"P*A =\\n{P @ A}\")"
      ],
      "id": "983b9f0a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## P, U, and L\n",
        "\n",
        "The $P$ matrix is a permutation matrix of “pivots” the others are\n",
        "triangular"
      ],
      "id": "aac8a36c-c9e6-4242-92e8-6c3dccfd5063"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P =\n",
            "[[0. 1.]\n",
            " [1. 0.]]\n",
            "L =\n",
            "[[1.  0. ]\n",
            " [0.5 1. ]]\n",
            "U =\n",
            "[[2. 4.]\n",
            " [0. 0.]]"
          ]
        }
      ],
      "source": [
        "print(f\"P =\\n{P}\")\n",
        "print(f\"L =\\n{L}\")\n",
        "print(f\"U =\\n{U}\")"
      ],
      "id": "bf14559d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LU Decompositions and Systems of Equations\n",
        "\n",
        "-   Pivoting is typically implied when talking about “LU”\n",
        "-   Used in the default `solve` algorithm (without more structure)\n",
        "-   Solving systems of equations with triangular matrices: for\n",
        "    $A x = L U x = b$\n",
        "    1.  Define $y = U x$\n",
        "    2.  Solve $L y = b$ for $y$ and $U x = y$ for $x$\n",
        "-   Since both are triangular, process is $O(N^2)$ (but LU itself\n",
        "    $O(N^3)$)\n",
        "-   Could be used to find `inv`\n",
        "    -   $A = L U$ then $A A^{-1} = I = L U A^{-1} = I$\n",
        "    -   Solve for $Y$ in $L Y = I$, then solve $U A^{-1} = Y$\n",
        "-   Tight connection to textbook Gaussian elimination (including\n",
        "    pivoting)\n",
        "\n",
        "## LU for Non-Singular Matrices"
      ],
      "id": "3228758c-15ff-422e-9dc6-0956fc8f29cc"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L*U =\n",
            "[[3. 4.]\n",
            " [1. 2.]]\n",
            "P*A =\n",
            "[[3. 4.]\n",
            " [1. 2.]]"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [3, 4]])\n",
        "P, L, U = lu(A)\n",
        "print(f\"L*U =\\n{L @ U}\")\n",
        "print(f\"P*A =\\n{P @ A}\")"
      ],
      "id": "291e2692"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## L, U, P"
      ],
      "id": "7bd6ebed-3de1-476f-8269-be199aeded53"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P =\n",
            "[[0. 1.]\n",
            " [1. 0.]]\n",
            "L =\n",
            "[[1.         0.        ]\n",
            " [0.33333333 1.        ]]\n",
            "U =\n",
            "[[3.         4.        ]\n",
            " [0.         0.66666667]]"
          ]
        }
      ],
      "source": [
        "print(f\"P =\\n{P}\")\n",
        "print(f\"L =\\n{L}\")\n",
        "print(f\"U =\\n{U}\")"
      ],
      "id": "e8c230a2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backwards Substitution Example\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "U x &= b\\\\\n",
        "U &\\equiv \\begin{bmatrix}\n",
        "3 & 1 \\\\\n",
        "0 & 2 \\\\\n",
        "\\end{bmatrix}, \\quad b = \\begin{bmatrix}\n",
        "7 \\\\\n",
        "2 \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Solving bottom row for $x_2$\n",
        "\n",
        "$$\n",
        "2 x_2 = 2,\\quad x_2 = 1\n",
        "$$\n",
        "\n",
        "Move up a row, solving for $x_1$, substituting for $x_2$\n",
        "\n",
        "$$\n",
        "3 x_1 + 1 x_2 = 7,\\quad 3 x_1 + 1 \\times 1 =  7,\\quad x_1 = 2\n",
        "$$\n",
        "\n",
        "Generalizes to many rows. For $L$ it is “forward substitution”\n",
        "\n",
        "## Use Triangular Structure if Possible\n",
        "\n",
        "-   Triangular matrices of size $N$ can be solved with back substitution\n",
        "    in $O(N^2)$\n",
        "-   Is $O(N^2)$ good or bad? Beats, $O(N^3)$ typical of general methods"
      ],
      "id": "a508fab9-0931-4027-a850-34bb3b5b4dce"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([2., 1.])"
            ]
          }
        }
      ],
      "source": [
        "U = np.array([[3, 1],\n",
        "              [0, 2]])\n",
        "b = np.array([7,2])\n",
        "solve(U,b) # works, but internally does an LU which is O(N^3)\n",
        "solve_triangular(U, b, lower=False) # fast O(N^2)"
      ],
      "id": "70e77130"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Symmetric and Positive Definite Matrices\n",
        "\n",
        "## Symmetric Matrix Structure\n",
        "\n",
        "Another common matrix type are symmetric, $A = A^{T}$"
      ],
      "id": "afa34da8-07ea-4caa-a160-1ee7fe735a29"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([-3.,  2.])"
            ]
          }
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 5]]) # also posdef, not singular\n",
        "b = np.array([1,4])\n",
        "# With scipy 1.11.3 check with scipy.linalg.issymmetric(A)\n",
        "solve(A, b, assume_a=\"sym\") # could also use \"pos\" since positive definite"
      ],
      "id": "580eecc6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Positive Definite Matrices\n",
        "\n",
        "-   A symmetric matrix $A$ is positive definite if $x^T A x > 0$ for all\n",
        "    $x \\neq 0$\n",
        "-   Useful in many areas, such as covariance matrices. Example"
      ],
      "id": "4533576d-a8b8-4f64-b2d5-40340a3ffa98"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x^T A x = 5"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 5]])\n",
        "x = np.array([0, 1]) # can't really check for all x\n",
        "print(f\"x^T A x = {x.T @ A @ x}\")"
      ],
      "id": "b12fda75"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Example of a symmetric matrix that is not positive definite"
      ],
      "id": "28e6a469-b873-467c-9d1a-0c0c04d110b0"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x^T A x = 0"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 0]])\n",
        "print(f\"x^T A x = {x.T @ A @ x}\")  # one counterexample is enough"
      ],
      "id": "21246f63"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   We can check these with eigenvalues\n",
        "\n",
        "## Cholesky Decomposition\n",
        "\n",
        "-   For symmetric positive definite matrices: $L = U^T$’\n",
        "-   Called a Cholesky decomposition: $A = L L^T$ for a lower triangular\n",
        "    matrix $L$.\n",
        "-   Equivalently, could find $A = U^T U$ for an upper triangular matrix\n",
        "    $U$"
      ],
      "id": "ba769650-2474-4076-8449-f7e8c0789588"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0.]\n",
            " [2. 1.]]\n",
            "L*L^T =\n",
            "[[1. 2.]\n",
            " [2. 5.]]"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 5]])\n",
        "L = cholesky(A, lower=True) # cholesky also defined for upper=True\n",
        "print(L)\n",
        "print(f\"L*L^T =\\n{L @ L.T}\")"
      ],
      "id": "d02fd31a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solving Positive Definite Systems"
      ],
      "id": "23f9f354-7762-4302-873b-3b1568a59d0b"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-3.  2.]\n",
            "[-3.  2.]"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 5]])\n",
        "b = np.array([1, 4])\n",
        "print(solve(A, b, assume_a=\"pos\")) # uses cholesky internally\n",
        "\n",
        "L = cholesky(A, lower=True)\n",
        "y = solve_triangular(L, b, lower=True)\n",
        "x = solve_triangular(L.T, y, lower=False)\n",
        "print(x)"
      ],
      "id": "4b0b0ace"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cholesky for Covariance Matrices\n",
        "\n",
        "-   Covariance matrices are positive-definite, semi-definite if\n",
        "    degenerate\n",
        "-   Key property of Gaussian random variables:\n",
        "    -   $X \\sim N(\\mu, \\Sigma)$ for\n",
        "        $\\mu \\in \\mathbb{R}^N, \\Sigma \\in \\mathbb{R}^{N \\times N}$\n",
        "    -   $X = \\mu + A Z$ for $Z \\sim N(0_N, I_N)$ where $A A^T = \\Sigma$\n",
        "-   That is, $A$ is the Cholesky decomposition of the covariance matrix\n",
        "\n",
        "## Matrices as Linear Transformations\n",
        "\n",
        "-   Recall: for $x \\in \\mathbb{R}^N$ we should think of a $f(x) = A x$\n",
        "    for $A\\in\\mathbb{R}^{M \\times N}$ as a linear transformation from\n",
        "    $\\mathbb{R}^N$ to $\\mathbb{R}^M$\n",
        "    -   Definition of Linear: $f(a x_1 + b x_2) = a f(x_1) + b f(x_2)$\n",
        "        for scalar $a,b$\n",
        "-   Similarly, the $y = f(x) = A x$ then $f^{-1}(y) = A^{-1} y$ goes\n",
        "    from $\\mathbb{R}^M$ to $\\mathbb{R}^N$\n",
        "    -   If the matrix is square and invertible, we can go back and forth\n",
        "        without losing information (i.e., bijective). Otherwise we may\n",
        "        be projected onto a lower-dimensional “manifold”.\n",
        "\n",
        "## Norms and Linear Transformations\n",
        "\n",
        "-   The vector norm $||x||_2$ is an important feature in many\n",
        "    applications\n",
        "\n",
        "    -   Hence $||f(x)||_2 = ||A x||_2$ frequently comes up in\n",
        "        Quantitative Economics and Datascience\n",
        "    -   e.g. linear regression is written as minimizing a vector norm\n",
        "\n",
        "    $$\n",
        "    \\min_{\\beta} ||y - X \\beta||_2\n",
        "    $$\n",
        "\n",
        "-   Matrix structure or decompositions of $A$ help us better understand\n",
        "    the $f(x)$ mapping\n",
        "\n",
        "# Orthogonal Matrices\n",
        "\n",
        "## Orthogonal Matrices — Definition\n",
        "\n",
        "-   A square matrix $Q$ is **orthogonal** if $Q^\\top Q = Q Q^\\top = I$\n",
        "    (equivalently, $Q^{-1} = Q^\\top$).\n",
        "    -   Columns are orthonormal: if\n",
        "        $Q = \\begin{bmatrix} q_1 \\; | \\; \\ldots \\; | \\; q_N \\end{bmatrix}$\n",
        "        then\n",
        "        -   $q_i \\cdot q_j = 0$ for $i \\neq j$  \n",
        "        -   $q_i \\cdot q_i = 1$.\n",
        "    -   Geometric action: combination of **rotation** and/or\n",
        "        **reflection**\n",
        "    -   Inverse action: if $y = Q x$, then $x = Q^\\top y$.\n",
        "-   Example (reflection):\n",
        "    $Q = \\begin{bmatrix}1 & 0 \\\\ 0 & -1\\end{bmatrix}$ flips the second\n",
        "    component while preserving $\\|\\cdot\\|_2$.\n",
        "\n",
        "## Orthogonal Matrices — Preserving Norms\n",
        "\n",
        "-   Rotation and reflection preserve lengths\n",
        "-   For all $x \\in \\mathbb{R}^N$, $\\|Q x\\|_2 = \\|x\\|_2$.\n",
        "-   Proof:\n",
        "    $\\|Qx\\|_2^2 = (Qx) \\cdot (Qx) = (Q x)^\\top Q x  = x^\\top Q^\\top Q x = x^\\top x = \\|x\\|_2^2$.\n",
        "\n",
        "## Orthogonal Matrices — Preserving Angles\n",
        "\n",
        "-   For any $x_1, x_2 \\in \\mathbb{R}^N$,\n",
        "    $(Q x_1) \\cdot (Q x_2) = x_1 \\cdot x_2$.\n",
        "-   Proof:\n",
        "    $(Q x_1) \\cdot (Q x_2) = (Q x_1)^\\top (Q x_2) = x_1^\\top Q^\\top Q x_2 = x_1^\\top x_2 = x_1 \\cdot x_2$.\n",
        "-   Orthogonal maps preserve inner products/“angles”\n",
        "    -   e.g., rotate both the $x_1$ and $x_2$ exactly the same amount.\n",
        "    -   Useful in many applications such as Principal Component Analysis\n",
        "        (PCA) and cosine similarity (which is used in NLP embeddings).\n",
        "\n",
        "# Eigenvalues and Eigenvectors\n",
        "\n",
        "## Eigenvalues and Eigenvectors\n",
        "\n",
        "-   For a square $A$, an eigenvector $x$ and eigenvalue $\\lambda$\n",
        "    satisfy $$\n",
        "    A x = \\lambda x\n",
        "    $$\n",
        "\n",
        "-   $A\\in\\mathbb{R}^{N\\times N}$ has $N$ eigenvalue/eigenvector pairs,\n",
        "    possible multiplicity of $\\lambda$\n",
        "\n",
        "-   Intuition: $x$ is a direction $A x \\propto x$ and $\\lambda$ says how\n",
        "    much it “stretches”\n",
        "\n",
        "## Properties of Eigenvalues and Eigenvectors\n",
        "\n",
        "-   For any eigenvector $x$ and scalar $c$ then $c x \\propto A x$ as\n",
        "    well\n",
        "-   Symmetric matrices have real eigenvalues and orthogonal\n",
        "    eigenvectors. i.e. $x_1 \\cdot x_2 = 0$ for $x_1 \\neq x_2$\n",
        "    eigenvectors. Complex in general\n",
        "-   Singular if and only if it has an eigenvalue of zero\n",
        "-   Positive (semi)definite if and only if all eigenvalues are strictly\n",
        "    (weakly) positive\n",
        "-   Diagonal matrix has eigenvalues as its diagonal\n",
        "-   Triangular matrix has eigenvalues as its diagonal\n",
        "\n",
        "## Positive Definite and Eigenvalues\n",
        "\n",
        "You cannot check $x^T A x > 0$ for all $x$. Check if “stretching” is\n",
        "positive"
      ],
      "id": "b241b446-5d17-408c-b666-4e7ad8703df5"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.23606798  4.23606798]\n",
            "pos-def? False\n",
            "pos-semi-def? False"
          ]
        }
      ],
      "source": [
        "A = np.array([[3, 2],\n",
        "              [2, 1]])\n",
        "# A_eigs = np.real(eigvals(A)) # symmetric matrices have real eigenvalues\n",
        "A_eigs = eigvalsh(A) # specialized for symmetric/hermitian matrices\n",
        "print(A_eigs)\n",
        "is_positive_definite = np.all(A_eigs > 0)\n",
        "is_positive_semi_definite = np.all(A_eigs >= 0) # or eigvals(A) >= -eps\n",
        "print(f\"pos-def? {is_positive_definite}\")\n",
        "print(f\"pos-semi-def? {is_positive_semi_definite}\")"
      ],
      "id": "eceb1289"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Positive Semi-Definite Matrices **May** Have a Zero Eigenvalue\n",
        "\n",
        "The simplest positive-semi-definite (but not posdef) matrix is"
      ],
      "id": "69297fe2-b06c-4cbf-92c8-31766895356f"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1.]\n",
            "pos-def? False\n",
            "pos-semi-def? True"
          ]
        }
      ],
      "source": [
        "A_eigs = eigvalsh(np.array([[1, 0],\n",
        "                            [0, 0]]))\n",
        "print(A_eigs)\n",
        "is_positive_definite = np.all(A_eigs > 0)\n",
        "is_positive_semi_definite = np.all(A_eigs >= 0) # or eigvals(A) >= -eps\n",
        "print(f\"pos-def? {is_positive_definite}\")\n",
        "print(f\"pos-semi-def? {is_positive_semi_definite}\")"
      ],
      "id": "f91afbfa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Eigen Decomposition (General)\n",
        "\n",
        "-   For real square matrix $A$ **we may be able to** take the\n",
        "    eigenvectors as columns of $Q$ (eigenvectors) and a diagonal matrix\n",
        "    $\\Lambda$ of the eigenvalues such that $$\n",
        "    A = Q \\, \\Lambda \\, Q^{-1}.\n",
        "    $$\n",
        "    -   $\\Lambda = \\operatorname{diag}(\\lambda_1,\\ldots,\\lambda_n)$\n",
        "        collects the eigenvalues (zeros allowed) into a diagonal matrix.\n",
        "    -   The eigenvectors and eigenvalues are complex in general\n",
        "-   These do not always exist. If so, you will hear the matrix is\n",
        "    **diagonalizable**.\n",
        "    -   Essentially, this is a question of whether the eigenvectors form\n",
        "        an $N$ dimensional basis and hence a $Q$ that is invertible.\n",
        "-   Alternatively, **every** real matrix admits a Singular Value\n",
        "    Decomposition (SVD)\n",
        "\n",
        "## Eigen Decomposition (Symmetric Case)\n",
        "\n",
        "-   A special case of this comes from the spectral theorem, which will\n",
        "    sometimes be referred to as a **spectral decomposition**\n",
        "-   For **any** real symmetric matrix $A$ (singular or not) there exist\n",
        "    an orthogonal $Q$ and a real diagonal $\\Lambda$ such that $$\n",
        "    A = Q \\, \\Lambda \\, Q^{\\top}.\n",
        "    $$\n",
        "    -   Orthogonal $Q^{\\top} Q = Q Q^{\\top} = I$ (so $Q^{-1}=Q^{\\top}$);\n",
        "        the columns of $Q$ form an **orthonormal basis** of\n",
        "        eigenvectors.\n",
        "    -   $\\Lambda = \\operatorname{diag}(\\lambda_1,\\ldots,\\lambda_n)$\n",
        "        collects the **real** eigenvalues (zeros allowed) into a\n",
        "        diagonal matrix. Complex in general for non-symmetric matrices,\n",
        "        but real for symmetric $A$.\n",
        "-   Geometric interpretation: apply $Q^{\\top}$ (rotation/reflection) to\n",
        "    rotate into the basis, scale by $\\Lambda$, then apply $Q$ to\n",
        "    rotate/reflect back.\n",
        "\n",
        "## Eigendecompositions and Matrix Powers\n",
        "\n",
        "-   Can be used to find $A^t$ for large $t$ (e.g. for Markov chains)\n",
        "    -   $P^t$, i.e. $P \\cdot P \\cdot \\ldots \\cdot P$ for $t$ times\n",
        "    -   $P = Q \\Lambda Q^{-1}$ then $P^t = Q \\Lambda^t Q^{-1}$ where\n",
        "        $\\Lambda^t$ is just the pointwise power\n",
        "-   Related tools such as SVD can help with dimensionality reduction\n",
        "\n",
        "## Spectral/Eigendecomposition of Symmetric Matrix Example"
      ],
      "id": "7d7e5f77-d66c-41b6-8813-6cff5f25f2d2"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eigenvectors are column-by-column in Q =\n",
            "[[-0.85065081 -0.52573111]\n",
            " [ 0.52573111 -0.85065081]]\n",
            "Q^T Q =\n",
            "[[1.0000000e+00 1.2127222e-17]\n",
            " [1.2127222e-17 1.0000000e+00]]\n",
            "Q^-1 == Q^T? True\n",
            "eigenvalues are in Lambda = [1.38196601+0.j 3.61803399+0.j]\n",
            "Q Lambda Q^T =\n",
            "[[2. 1.]\n",
            " [1. 3.]]"
          ]
        }
      ],
      "source": [
        "A = np.array([[2, 1],\n",
        "              [1, 3]])\n",
        "Lambda, Q = eig(A)\n",
        "print(f\"eigenvectors are column-by-column in Q =\\n{Q}\")\n",
        "print(f\"Q^T Q =\\n{Q.T @ Q}\") # should be I\n",
        "print(f\"Q^-1 == Q^T? {np.allclose(np.linalg.inv(Q), Q.T)}\") # should be True\n",
        "print(f\"eigenvalues are in Lambda = {Lambda}\")\n",
        "print(f\"Q Lambda Q^T =\\n{Q @ np.diag(np.real(Lambda)) @ Q.T}\")"
      ],
      "id": "d4f4a722"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spectral/Eigendecomposition of an Asymmetric Matrix Example"
      ],
      "id": "aa5e44ab-1131-40f9-87c3-48f88dad4425"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eigenvectors are column-by-column in Q =\n",
            "[[ 0.80689822 -0.34372377]\n",
            " [ 0.59069049  0.9390708 ]]\n",
            "Q^-1 == Q^T? False\n",
            "eigenvalues are in Lambda = [5.73205081+0.j 2.26794919+0.j]\n",
            "Q Lambda inv(Q) =\n",
            "[[5. 1.]\n",
            " [2. 3.]]"
          ]
        }
      ],
      "source": [
        "A = np.array([[5, 1],\n",
        "              [2, 3]])\n",
        "Lambda, Q = eig(A)\n",
        "print(f\"eigenvectors are column-by-column in Q =\\n{Q}\")\n",
        "print(f\"Q^-1 == Q^T? {np.allclose(np.linalg.inv(Q), Q.T)}\") # should be False\n",
        "print(f\"eigenvalues are in Lambda = {Lambda}\")\n",
        "print(f\"Q Lambda inv(Q) =\\n{Q @ np.diag(np.real(Lambda)) @ np.linalg.inv(Q)}\")"
      ],
      "id": "ed7ffc7c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spectral Radius is Maximum Absolute Eigenvalue\n",
        "\n",
        "-   If any $\\lambda \\in \\Lambda$ are $> 1$ can see this would explode\n",
        "-   Useful for seeing if iteration $x_{t+1} = A x_t$ from a $x_0$\n",
        "    explodes\n",
        "-   The **spectral radius** of matrix $A$ is\n",
        "\n",
        "$$\n",
        "  \\rho(A) = \\max_{\\lambda \\in \\Lambda}|\\lambda|\n",
        "  $$\n",
        "\n",
        "## Recall Condition Numbers\n",
        "\n",
        "-   The condition number of the matrix $A$ is the ratio of the largest\n",
        "    and smallest eigenvalues\n",
        "    -   $\\kappa(A) = \\left|\\frac{\\lambda_{\\text{max}}}{\\lambda_{\\text{min}}}\\right|$\n",
        "        for $\\lambda$ the eigenvalues of $A$.\n",
        "    -   Sense of relative dispersion and consistent scaling\n",
        "-   For example, using the fact that the triangular matrix has\n",
        "    eigenvalues on the diagonal $$\n",
        "    A = \\begin{bmatrix}\n",
        "    1 & 2 \\\\\n",
        "    0 & 0.001\n",
        "    \\end{bmatrix}\n",
        "    $$\n",
        "    -   $\\kappa(A) = 1/0.001 = 1000$ and $\\rho(A) = 1$.\n",
        "-   Or, the best we can do is $\\kappa(A) = 1$ for the identity (or\n",
        "    triangular with 1’s on diagonal) $$\n",
        "    A = \\begin{bmatrix}\n",
        "    1 & 0 \\\\\n",
        "    0 & 1\n",
        "    \\end{bmatrix}\n",
        "    $$\n",
        "\n",
        "# (Optional) Matrix Conditioning and Stability\n",
        "\n",
        "## Matrix Conditioning\n",
        "\n",
        "-   Poorly conditioned matrices can lead to inaccurate or wrong\n",
        "    solutions\n",
        "-   Can happen when problem is at radically different scales - which can\n",
        "    sometimes be fixed by demeaning, rescaling, etc.\n",
        "-   Also tends to happen when matrices are close to singular"
      ],
      "id": "890baaf6-384b-4626-8def-ed84cfaa9594"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A =\n",
            "[[1.        1.       ]\n",
            " [1.0000001 1.       ]]\n",
            "A^-1 =\n",
            "[[-9999999.99336215  9999999.99336215]\n",
            " [10000000.99336215 -9999999.99336215]]"
          ]
        }
      ],
      "source": [
        "eps = 1e-7\n",
        "A = np.array([[1, 1],\n",
        "              [1 + eps, 1]])\n",
        "print(f\"A =\\n{A}\")\n",
        "print(f\"A^{-1} =\\n{inv(A)}\")"
      ],
      "id": "2b93b118"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reminder: Condition Numbers of Matrices\n",
        "\n",
        "-   $\\text{cond}(A) = \\left|\\frac{\\lambda_{max}}{\\lambda_{min}}\\right|$\n",
        "-   Scale free measure of numerical issues for a variety of matrix\n",
        "    operations\n",
        "-   Intuition: if $\\text{cond}(A) = K$, then $b \\to b + \\nabla b$ change\n",
        "    in $b$ amplifies to a $x \\to x + K \\nabla b$ error when solving\n",
        "    $A x = b$.\n",
        "-   See [Matlab Docs on\n",
        "    inv](https://www.mathworks.com/help/matlab/ref/inv.html#bu6sfy8-1)\n",
        "    for example, where `inv` is a bad idea due to poor conditioning"
      ],
      "id": "8e4ce0b6-6688-4a92-a301-95efa1cbd430"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "condition(I) = 1.0\n",
            "condition(A) = 40000001.962777555, condition(A^(-1)) = 40000002.02779216"
          ]
        }
      ],
      "source": [
        "print(f\"condition(I) = {cond(np.eye(2))}\")\n",
        "print(f\"condition(A) = {cond(A)}, condition(A^(-1)) = {cond(inv(A))}\")"
      ],
      "id": "4e57ab32"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example with Interpolation\n",
        "\n",
        "-   Consider fitting data $x\\in\\mathbb{R}^{N+1}$ and\n",
        "    $y\\in\\mathbb{R}^{N+1}$ with an $N$-degree polynomial\n",
        "-   That is, find $c \\in \\mathbb{R}^{N+1}$ such that\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "c_0 + c_1 x_1 + c_2 x_1^2 + \\ldots + c_N x_1^N &= y_1\\\\\n",
        "\\vdots & \\\\\n",
        "c_0 + c_1 x_N + c_2 x_N^2 + \\ldots + c_N x_N^N &= y_N\\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "-   Which we can then use as $P(x) = \\sum_{n=0}^N c_n x^n$ to\n",
        "    interpolate between the points\n",
        "\n",
        "## Writing as a Linear System\n",
        "\n",
        "-   Define a matrix of all of the powers of the $x$ values\n",
        "\n",
        "$$\n",
        "A \\equiv \\begin{bmatrix}\n",
        "1 & x_0 & x_0^2 & \\ldots & x_0^N\\\\\n",
        "\\vdots & \\vdots &\\vdots & \\vdots & \\vdots\\\\\n",
        "1 & x_N & x_N^2 & \\ldots & x_N^N\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "-   Then solve for $c$ as the solution (where $A$ is invertible if $x_n$\n",
        "    are unique)\n",
        "\n",
        "$$\n",
        "A c = y\n",
        "$$\n",
        "\n",
        "## Solving an Example\n",
        "\n",
        "-   Solve and look at numerical error of interpolation using\n",
        "    $||x||_{\\infty} = \\max_n |x_n|$ (i.e., inf-norm)\n",
        "-   We already see a difference in error between using `solve` and `inv`"
      ],
      "id": "6a68825b-9369-44ed-b932-b6f5d6129f62"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error = 1.574562702444382e-11, error using inv(A) = 1.1932570487260818e-09\n",
            "cond(A) = 564652.3214000753"
          ]
        }
      ],
      "source": [
        "N = 5\n",
        "x = np.linspace(0.0, 10.0, N + 1)\n",
        "y = np.exp(x)  # example function to interpolate\n",
        "A = np.array([[x_i**n for n in range(N + 1)] for x_i in x])  # or np.vander\n",
        "c = solve(A, y)\n",
        "c_inv = inv(A) @ y\n",
        "print(f\"error = {norm(A @ c - y, np.inf)}, \\\n",
        "error using inv(A) = {norm(A @ c_inv - y, np.inf)}\")\n",
        "print(f\"cond(A) = {cond(A)}\")"
      ],
      "id": "40b2bf1e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Things Getting Poorly Conditioned Quickly"
      ],
      "id": "56058cae-22bc-49b4-84ad-d046322c3993"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error = 5.334186425898224e-10, error using inv(A) = 6.22717197984457e-06\n",
            "cond(A) = 4462824600195.809"
          ]
        }
      ],
      "source": [
        "N = 10\n",
        "x = np.linspace(0.0, 10.0, N + 1)\n",
        "y = np.exp(x)  # example function to interpolate\n",
        "A = np.array([[x_i**n for n in range(N + 1)] for x_i in x])  # or np.vander\n",
        "c = solve(A, y)\n",
        "c_inv = inv(A) @ y # Solving with inv(A) instead of solve(A, y)\n",
        "print(f\"error = {norm(A @ c - y, np.inf)}, \\\n",
        "error using inv(A) = {norm(A @ c_inv - y, np.inf)}\")\n",
        "print(f\"cond(A) = {cond(A)}\")"
      ],
      "id": "97304c35"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Matrix Inverses Fail Completely for $N = 20$\n",
        "\n",
        "-   You do not need massive matrices to get into trouble"
      ],
      "id": "817574bb-ce5d-4c72-86ee-3c7d0b436031"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error = 8.36735125631094e-10, error using inv(A) = 1419.6725472353137\n",
            "cond(A) = 2.938e+24"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/runner/work/grad_econ_datascience/grad_econ_datascience/.venv/lib/python3.13/site-packages/scipy/_lib/_util.py:1233: LinAlgWarning: Ill-conditioned matrix (rcond=6.72948e-28): result may not be accurate.\n",
            "  return f(*arrays, *other_args, **kwargs)"
          ]
        }
      ],
      "source": [
        "N = 20\n",
        "x = np.linspace(0.0, 10.0, N + 1)\n",
        "y = np.exp(x)  # example function to interpolate\n",
        "A = np.array([[x_i**n for n in range(N + 1)] for x_i in x])  # or np.vander\n",
        "c = solve(A, y)\n",
        "c_inv = inv(A) @ y # Solving with inv(A) instead of solve(A, y)\n",
        "print(f\"error = {norm(A @ c - y, np.inf)}, \\\n",
        "error using inv(A) = {norm(A @ c_inv - y, np.inf)}\")\n",
        "print(f\"cond(A) = {cond(A):.4g}\")"
      ],
      "id": "d4261e13"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Moral of this Story\n",
        "\n",
        "-   Use `solve`, which is faster and can often solve ill-conditioned\n",
        "    problems. Rarely use `inv`, and only when you know the problem is\n",
        "    well-conditioned\n",
        "-   Check conditioning of matrices when doing numerical work as an\n",
        "    occasional diagnostic, as it is a good indicator of potential\n",
        "    problems and collinearity\n",
        "-   For approximation, never use a monomial basis for polynomials\n",
        "    -   Prefer polynomials like Chebyshev, which are as orthogonal as\n",
        "        possible"
      ],
      "id": "ff076b54-0bea-4fc8-b634-cca17444455d"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cond(A) = 3.64e+09, cond(A_monomial) = 5.311e+17"
          ]
        }
      ],
      "source": [
        "N = 40\n",
        "x = np.linspace(-1, 1, N+1)  # Or any other range of x values\n",
        "A = np.array([[np.polynomial.Chebyshev.basis(n)(x_i) for n in range(N+1)] for x_i in x])\n",
        "A_monomial = np.array([[x_i**n for n in range(N + 1)] for x_i in x])  # or np.vander\n",
        "print(f\"cond(A) = {cond(A):.4g}, cond(A_monomial) = {cond(A_monomial):.4g}\")"
      ],
      "id": "a1751448"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/home/runner/work/grad_econ_datascience/grad_econ_datascience/.venv/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  }
}