{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Foundations of Numerical Linear Algebra\n",
        "\n",
        "Graduate Quantitative Economics and Datascience\n",
        "\n",
        "Jesse Perla (University of British Columbia)\n",
        "\n",
        "## Going Beyond “`reg y x, robust`”\n",
        "\n",
        "-   Data science, econometrics, and macroeconomics are built on linear\n",
        "    algebra.\n",
        "-   Numerical linear algebra has all sorts of pitfalls, which become\n",
        "    more critical as we scale up to larger problems.\n",
        "-   Speed differences in choosing better algorithms can be orders of\n",
        "    magnitude.\n",
        "-   Crucial to know what goes on under-the-hood in Stata/R/python\n",
        "    packages for applied work, even if you don’t implement it yourself.\n",
        "\n",
        "## Extra Materials\n",
        "\n",
        "-   Material related to: [QuantEcon\n",
        "    Python](https://python.quantecon.org/linear_algebra.html),\n",
        "    [QuantEcon Data\n",
        "    Science](https://datascience.quantecon.org/scientific/applied_linalg.html),\n",
        "    [Intro Quantitative Economics with\n",
        "    Python](https://intro.quantecon.org/)\n",
        "-   **Self-study and Optional Materials:**\n",
        "    -   [Basics of linear algebra, matrices, norms, and linear\n",
        "        independence](https://python.quantecon.org/linear_algebra.html)\n",
        "    -   [Numerical\n",
        "        optimization](https://datascience.quantecon.org/scientific/optimization.html)\n",
        "    -   [Systems of\n",
        "        Equations](https://python.quantecon.org/linear_algebra.html#solving-systems-of-equations)\n",
        "    -   [Eigenvectors and\n",
        "        eigenvalues](https://python.quantecon.org/linear_algebra.html#eigenvalues-and-eigenvectors)\n",
        "    -   [Downloading and manipulating data in\n",
        "        Python](https://intro.quantecon.org/long_run_growth.html) and\n",
        "        [here](https://intro.quantecon.org/business_cycle.html)\n",
        "    -   [Introductory material on linear\n",
        "        algebra](https://intro.quantecon.org/linear_equations.html) and\n",
        "        [more](https://datascience.quantecon.org/scientific/applied_linalg.html)\n",
        "    -   [Matrix decompositions and other\n",
        "        topics](https://python.quantecon.org/linear_algebra.html#further-topics)\n",
        "\n",
        "## Packages\n",
        "\n",
        "This section uses the following packages:"
      ],
      "id": "dcee6f67-db27-4de0-844f-968c9b2641da"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from numpy.linalg import cond, matrix_rank, norm\n",
        "from scipy.linalg import inv, solve, det, eig, lu, eigvals\n",
        "from scipy.linalg import solve_triangular, eigvalsh, cholesky"
      ],
      "id": "97d3d2b6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Computational Complexity\n",
        "\n",
        "**Big-O Notation**\n",
        "\n",
        "For a function $f(N)$ and a positive constant $C$, we say $f(N)$ is\n",
        "$O(g(N))$, if there exist positive constants $C$ and $N_0$ such that:\n",
        "\n",
        "$$\n",
        "0 \\leq f(N) \\leq C \\cdot g(N) \\quad \\text{for all } N \\geq N_0\n",
        "$$\n",
        "\n",
        "-   Often crucial to know how problems scale asymptotically (as\n",
        "    $N\\to\\infty$)\n",
        "-   Caution! This is only an asymptotic limit, and can be misleading for\n",
        "    small $N$\n",
        "    -   $f_1(N) = N^3 + N$ is $O(N^3)$\n",
        "    -   $f_2(N) = 1000 N^2 + 3 N$ is $O(N^2)$\n",
        "    -   For roughly $N>1000$ use $f_2$ algorithm, otherwise $f_1$\n",
        "\n",
        "## Examples of Computational Complexity\n",
        "\n",
        "-   Simple examples:\n",
        "    -   $x \\cdot y = \\sum_{n=1}^N x_n y_n$ is $O(N)$ since it requires\n",
        "        $N$ multiplications and additions\n",
        "    -   $A x$ for $A\\in\\mathbb{R}^{N\\times N},x\\in\\mathbb{R}^N$ is\n",
        "        $O(N^2)$ since it requires $N$ dot products, each $O(N)$\n",
        "\n",
        "## Numerical Precision\n",
        "\n",
        "**Machine Epsilon**\n",
        "\n",
        "For a given datatype, $\\epsilon$ is defined as\n",
        "$\\epsilon = \\min_{\\delta > 0} \\left\\{ \\delta : 1 + \\delta > 1 \\right\\}$\n",
        "\n",
        "-   Computers have finite precision. 64-bit typical, but 32-bit on GPUs"
      ],
      "id": "9f0b931c-ed43-4151-9e6b-c43c79fc9502"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "machine epsilon for float64 = 2.220446049250313e-16\n",
            "1 + eps/2 == 1? True\n",
            "machine epsilon for float32 = 1.1920928955078125e-07"
          ]
        }
      ],
      "source": [
        "print(f\"machine epsilon for float64 = {np.finfo(float).eps}\")\n",
        "print(f\"1 + eps/2 == 1? {1.0 + 1.1e-16 == 1.0}\")\n",
        "print(f\"machine epsilon for float32 = {np.finfo(np.float32).eps}\")"
      ],
      "id": "9872ebfa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basic Linear Algebra\n",
        "\n",
        "## Norms\n",
        "\n",
        "-   Common measure of size is the Euclidean norm, or $L^2$ norm for\n",
        "    $x \\in \\mathbb{R}^2$\n",
        "-   Complexity is $O(N)$, square $N$ times then $N$ additions $$\n",
        "    ||x||_2 = \\sqrt{\\sum_{n=1}^N x_n^2}\n",
        "    $$"
      ],
      "id": "e13829f0-e5ef-4da4-9968-325d947e7fc7"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.7416573867739413\n",
            "3.7416573867739413\n",
            "3.7416573867739413\n",
            "||x||_2^2 = 14.0 = 14 = 14"
          ]
        }
      ],
      "source": [
        "x = np.array([1, 2, 3]) # Calculating different ways (in order of preference)\n",
        "print(np.sqrt(sum(xval**2 for xval in x))) # manual with comprehensions\n",
        "print(np.sqrt(np.sum(np.square(x)))) # broadcasts\n",
        "print(norm(x)) # built-in to numpy norm(x, ord=2) alternatively\n",
        "print(f\"||x||_2^2 = {norm(x)**2} = {x.T @ x} = {np.dot(x, x)}\")"
      ],
      "id": "15b30283"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solving Systems of Equations\n",
        "\n",
        "-   Solving $A x = b$ for $x$ is equivalent $A^{-1} A x = A^{-1} b$\n",
        "-   Then since $A^{-1}A = I$, and $I x = x$, we have $x = A^{-1} b$\n",
        "-   Careful since matrix algebra is not commutative!"
      ],
      "id": "29c41544-02d4-4068-aef3-add8c1c9b676"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([-1.,  1.])"
            ]
          }
        }
      ],
      "source": [
        "A = np.array([[0, 2],\n",
        "              [3, 4]]) # or ((0, 2), (3, 4))\n",
        "b = np.array([2,1])  # Column vector\n",
        "x = solve(A, b)  # Solve Ax = b for x\n",
        "x"
      ],
      "id": "eeacd641"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using the Inverse Directly\n",
        "\n",
        "-   Can replace the `solve` with a calculation of an inverse\n",
        "-   But it can be slower or less accurate than solving the system\n",
        "    directly"
      ],
      "id": "4f3e08ee-a8df-4c71-8aaf-ffd26f5ad38d"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([-1.,  1.])"
            ]
          }
        }
      ],
      "source": [
        "A_inv = inv(A)\n",
        "A_inv @ b # i.e, A^{-1} * b"
      ],
      "id": "024853ca"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Combinations\n",
        "\n",
        "We can think of solving a system as finding the linear combination of\n",
        "columns of $A$ that equal $b$"
      ],
      "id": "d17b95cf-2afe-4f7b-9f2d-c13e0f4bbe94"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b = [2 1], b_star = [2. 1.]"
          ]
        }
      ],
      "source": [
        "b_star = x[0] * A[:, 0] + x[1] * A[:, 1] # using x solution\n",
        "print(f\"b = {b}, b_star = {b_star}\")"
      ],
      "id": "a5163a8c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Column Space and Rank\n",
        "\n",
        "-   The column space of a matrix represents all possible linear\n",
        "    combinations of its columns.\n",
        "-   It forms a basis for the space of solutions when solving systems of\n",
        "    linear equations represented by the matrix\n",
        "-   The rank of a matrix is the dimension of its column space"
      ],
      "id": "d5be4e47-ccb8-4690-92bf-feff3c5b01b9"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "np.int64(2)"
            ]
          }
        }
      ],
      "source": [
        "A = np.array([[0, 2],\n",
        "              [3, 4]])\n",
        "matrix_rank(A)"
      ],
      "id": "6b88d55d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hence, can solve $A x = b$ for any $b \\in \\mathbb{R}^2$ since the column\n",
        "space is the entire space $\\mathbb{R}^2$\n",
        "\n",
        "## Singular Matrices\n",
        "\n",
        "On the other hand, note"
      ],
      "id": "e80c8246-f809-4cb6-8d25-3e7806fa3ddd"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "np.int64(1)"
            ]
          }
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 4]])\n",
        "matrix_rank(A)"
      ],
      "id": "fe4102ea"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So we can only solve $A x = b$ for\n",
        "$b \\propto \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} \\propto \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$\n",
        "\n",
        "## Checking Singularity"
      ],
      "id": "2f7a404a-62f7-4f9a-9f4c-c5841d5cd5fc"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "Matrix is singular (non-invertible)."
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 4]])\n",
        "# An (expensive) way to check if A is singular is if det(A) = 0\n",
        "print(det(A) == 0.0)\n",
        "print(matrix_rank(A) != A.shape[0]) # or check rank\n",
        "# Check before inverting or use exceptions\n",
        "try:\n",
        "    inv(A)\n",
        "    print(\"Matrix is not singular (invertible).\")\n",
        "except np.linalg.LinAlgError:\n",
        "    print(\"Matrix is singular (non-invertible).\")"
      ],
      "id": "e6cb994e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Determinant is Not Scale Invariant\n",
        "\n",
        "-   Reminder: numerical precision in calculations makes it hard to\n",
        "    compare to zero\n",
        "-   The determinate is useful but depends on the scale of the matrix\n",
        "-   A more robust alternative is the condition number (more next\n",
        "    lecture)"
      ],
      "id": "0c41afd0-360f-40d4-846e-58d714b95f68"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "det(A)=-1e-08, det(K*A)=-100\n",
            "cond(A)=1e+09, cond(K*A)=1e+09,\n",
            "det(inv(A))=-1e+08, cond(inv(A))=1e+09"
          ]
        }
      ],
      "source": [
        "eps, K = 1e-8, 100000\n",
        "A = np.array([[1, 2],\n",
        "              [1 + eps, 2 + eps]]) # not 1 + eps, 2(1+eps)!\n",
        "print(f\"det(A)={det(A):.5g}, det(K*A)={det(K*A):.5g}\")\n",
        "print(f\"cond(A)={cond(A):.5g}, cond(K*A)={cond(K*A):.5g},\")\n",
        "print(f\"det(inv(A))={det(inv(A)):.5g}, cond(inv(A))={cond(inv(A)):.5g}\")"
      ],
      "id": "7a6bfc38"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpreting Condition Numbers\n",
        "\n",
        "-   The condition number of the matrix $A$ is\n",
        "    $\\kappa(A) = ||A|| \\cdot ||A^{-1}||$, which can be shown in terms of\n",
        "    ratio of the largest and smallest eigenvalues\n",
        "    -   $\\kappa(A) = \\left| \\frac{\\lambda_{\\text{max}}}{\\lambda_{\\text{min}}} \\right|$\n",
        "        for $\\lambda$ the eigenvalues of $A$. More soon!\n",
        "-   Crude intuition: for machine epsilon $\\epsilon_{\\text{mach}}$ when\n",
        "    calculating some $x$\n",
        "    -   The relative error, $||x - x_{\\text{approx}}||/||x||$ is roughly\n",
        "        $\\kappa(A) \\cdot \\epsilon_{\\text{mach}}$\n",
        "    -   Solving $A x = b$ when $\\epsilon_{\\text{mach}} = 1e^{-16}$ it\n",
        "        amplifies errors in $b$, etc.\n",
        "    -   if $\\kappa(A) \\approx 1e^{16}$ errors amplified so the scale of\n",
        "        100% relative error\n",
        "\n",
        "## Rules of Thumb\n",
        "\n",
        "-   Rule of thumb for standard floating points where\n",
        "    $\\epsilon_{\\text{mach}} = 1e^{-16}$:\n",
        "    -   $\\kappa(A) \\approx 1$ well-conditioned\n",
        "    -   $\\kappa(A) < 100$ fairly well-conditioned\n",
        "    -   $\\kappa(A) < 1e^{5}$ moderately ill-conditioned. Take care\n",
        "    -   $\\kappa(A) < 1e^{8}$ ill-conditioned and might introduce\n",
        "        significant errors, especially in algorithms which repeatedly\n",
        "        use the same calculations\n",
        "    -   $\\kappa(A) > 1e^{8}$ very ill-conditioned and likely to\n",
        "        introduce significant errors\n",
        "-   Choose solution algorithms based on “numerical stability” and\n",
        "    “conditioning” when worried\n",
        "-   Much more extreme with 32-bit floats such as when using GPUs.\n",
        "\n",
        "# Solving Linear Systems of Equations\n",
        "\n",
        "## Solving Systems with Multiple RHS\n",
        "\n",
        "-   Inverse is nice because you can reuse the $A^{-1}$ to solve\n",
        "    $A x = b$ for many $b$\n",
        "-   However, you can do this with `solve` as well\n",
        "-   Or can reuse LU factorizations (discussed next)"
      ],
      "id": "f5abcd1b-c0ab-489b-b700-e0421d89a83f"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.         -1.33333333]\n",
            " [ 1.          1.5       ]]\n",
            "Checking: A*[-1.  1.] = [2. 1.] = [2 1], column of B"
          ]
        }
      ],
      "source": [
        "A = np.array([[0, 2],\n",
        "              [3, 4]])\n",
        "B = np.array([[2, 3],\n",
        "              [1, 2]])  # [2,1] and [3,2] as columns\n",
        "# or: B = np.column_stack([np.array([2, 1]),np.array([3,2])])\n",
        "X = solve(A, B)  # Solve AX = B for X\n",
        "print(X)\n",
        "print(f\"Checking: A*{X[:,0]} = {A@X[:, 0]} = {B[:,0]}, column of B\")"
      ],
      "id": "2b8e8753"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LU(P) Decompositions\n",
        "\n",
        "-   We can “factor” any square $A$ into $P A = L U$ for triangular $L$\n",
        "    and $U$. Invertible can have $A = L U$, called the LU decomposition.\n",
        "    “P” is for partial-pivoting\n",
        "-   Singular matrices may not have full-rank $L$ or $U$ matrices"
      ],
      "id": "09b65a2e-d078-4133-a55f-a6ead2fd467d"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L*U =\n",
            "[[2. 4.]\n",
            " [1. 2.]]\n",
            "P*A =\n",
            "[[2. 4.]\n",
            " [1. 2.]]"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 4]])\n",
        "P, L, U = lu(A)\n",
        "print(f\"L*U =\\n{L @ U}\")\n",
        "print(f\"P*A =\\n{P @ A}\")"
      ],
      "id": "8c196ebe"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## P, U, and L\n",
        "\n",
        "The $P$ matrix is a permutation matrix of “pivots” the others are\n",
        "triangular"
      ],
      "id": "453d1fb0-b805-4bab-b266-1964b95cce88"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P =\n",
            "[[0. 1.]\n",
            " [1. 0.]]\n",
            "L =\n",
            "[[1.  0. ]\n",
            " [0.5 1. ]]\n",
            "U =\n",
            "[[2. 4.]\n",
            " [0. 0.]]"
          ]
        }
      ],
      "source": [
        "print(f\"P =\\n{P}\")\n",
        "print(f\"L =\\n{L}\")\n",
        "print(f\"U =\\n{U}\")"
      ],
      "id": "1f14808f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LU Decompositions and Systems of Equations\n",
        "\n",
        "-   Pivoting is typically implied when talking about “LU”\n",
        "-   Used in the default `solve` algorithm (without more structure)\n",
        "-   Solving systems of equations with triangular matrices: for\n",
        "    $A x = L U x = b$\n",
        "    1.  Define $y = U x$\n",
        "    2.  Solve $L y = b$ for $y$ and $U x = y$ for $x$\n",
        "-   Since both are triangular, process is $O(N^2)$ (but LU itself\n",
        "    $O(N^3)$)\n",
        "-   Could be used to find `inv`\n",
        "    -   $A = L U$ then $A A^{-1} = I = L U A^{-1} = I$\n",
        "    -   Solve for $Y$ in $L Y = I$, then solve $U A^{-1} = Y$\n",
        "-   Tight connection to textbook Gaussian elimination (including\n",
        "    pivoting)\n",
        "\n",
        "## LU for Non-Singular Matrices"
      ],
      "id": "7cff1ede-b87a-40ff-861e-88486543f9de"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L*U =\n",
            "[[3. 4.]\n",
            " [1. 2.]]\n",
            "P*A =\n",
            "[[3. 4.]\n",
            " [1. 2.]]"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [3, 4]])\n",
        "P, L, U = lu(A)\n",
        "print(f\"L*U =\\n{L @ U}\")\n",
        "print(f\"P*A =\\n{P @ A}\")"
      ],
      "id": "f4d70bee"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## L, U, P"
      ],
      "id": "d3791b5e-d31b-4bc4-bd64-77def89f81cb"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P =\n",
            "[[0. 1.]\n",
            " [1. 0.]]\n",
            "L =\n",
            "[[1.         0.        ]\n",
            " [0.33333333 1.        ]]\n",
            "U =\n",
            "[[3.         4.        ]\n",
            " [0.         0.66666667]]"
          ]
        }
      ],
      "source": [
        "print(f\"P =\\n{P}\")\n",
        "print(f\"L =\\n{L}\")\n",
        "print(f\"U =\\n{U}\")"
      ],
      "id": "81d42453"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backwards Substitution Example\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "U x &= b\\\\\n",
        "U &\\equiv \\begin{bmatrix}\n",
        "3 & 1 \\\\\n",
        "0 & 2 \\\\\n",
        "\\end{bmatrix}, \\quad b = \\begin{bmatrix}\n",
        "7 \\\\\n",
        "2 \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Solving bottom row for $x_2$\n",
        "\n",
        "$$\n",
        "2 x_2 = 2,\\quad x_2 = 1\n",
        "$$\n",
        "\n",
        "Move up a row, solving for $x_1$, substituting for $x_2$\n",
        "\n",
        "$$\n",
        "3 x_1 + 1 x_2 = 7,\\quad 3 x_1 + 1 \\times 1 =  7,\\quad x_1 = 2\n",
        "$$\n",
        "\n",
        "Generalizes to many rows. For $L$ it is “forward substitution”\n",
        "\n",
        "## Use Triangular Structure if Possible\n",
        "\n",
        "-   Triangular matrices of size $N$ can be solved with back substitution\n",
        "    in $O(N^2)$\n",
        "-   Is $O(N^2)$ good or bad? Beats, $O(N^3)$ typical of general methods"
      ],
      "id": "a98bf3d7-417c-4dfe-a055-d60ae1474aa2"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([2., 1.])"
            ]
          }
        }
      ],
      "source": [
        "U = np.array([[3, 1],\n",
        "              [0, 2]])\n",
        "b = np.array([7,2])\n",
        "solve(U,b) # works, but internally does an LU which is O(N^3)\n",
        "solve_triangular(U, b, lower=False) # fast O(N^2)"
      ],
      "id": "3476c44d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Symmetric and Positive Definite Matrices\n",
        "\n",
        "## Symmetric Matrix Structure\n",
        "\n",
        "Another common matrix type are symmetric, $A = A^{T}$"
      ],
      "id": "f601fe13-0d9a-46bd-b5ef-342c79f504a4"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([-3.,  2.])"
            ]
          }
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 5]]) # also posdef, not singular\n",
        "b = np.array([1,4])\n",
        "# With scipy 1.11.3 check with scipy.linalg.issymmetric(A)\n",
        "solve(A, b, assume_a=\"sym\") # could also use \"pos\" since positive definite"
      ],
      "id": "216c653b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Positive Definite Matrices\n",
        "\n",
        "-   A symmetric matrix $A$ is positive definite if $x^T A x > 0$ for all\n",
        "    $x \\neq 0$\n",
        "-   Useful in many areas, such as covariance matrices. Example"
      ],
      "id": "62440e89-c3de-4f5d-b5e4-232ad2f569ea"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x^T A x = 5"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 5]])\n",
        "x = np.array([0, 1]) # can't really check for all x\n",
        "print(f\"x^T A x = {x.T @ A @ x}\")"
      ],
      "id": "407b676f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Example of a symmetric matrix that is not positive definite"
      ],
      "id": "517d86de-e8eb-4334-a150-5006d6ceda2e"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x^T A x = 0"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 0]])\n",
        "print(f\"x^T A x = {x.T @ A @ x}\")  # one counterexample is enough"
      ],
      "id": "d898fd05"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   We can check these with eigenvalues\n",
        "\n",
        "## Cholesky Decomposition\n",
        "\n",
        "-   For symmetric positive definite matrices: $L = U^T$’\n",
        "-   Called a Cholesky decomposition: $A = L L^T$ for a lower triangular\n",
        "    matrix $L$.\n",
        "-   Equivalently, could find $A = U^T U$ for an upper triangular matrix\n",
        "    $U$"
      ],
      "id": "a88a7c90-214d-46c9-b256-746d162637c1"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0.]\n",
            " [2. 1.]]\n",
            "L*L^T =\n",
            "[[1. 2.]\n",
            " [2. 5.]]"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 5]])\n",
        "L = cholesky(A, lower=True) # cholesky also defined for upper=True\n",
        "print(L)\n",
        "print(f\"L*L^T =\\n{L @ L.T}\")"
      ],
      "id": "d8b293a4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solving Positive Definite Systems"
      ],
      "id": "7887160b-d1b8-4dd2-a329-aaec4c664266"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-3.  2.]\n",
            "[-3.  2.]"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 5]])\n",
        "b = np.array([1, 4])\n",
        "print(solve(A, b, assume_a=\"pos\")) # uses cholesky internally\n",
        "\n",
        "L = cholesky(A, lower=True)\n",
        "y = solve_triangular(L, b, lower=True)\n",
        "x = solve_triangular(L.T, y, lower=False)\n",
        "print(x)"
      ],
      "id": "99ea3c9a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cholesky for Covariance Matrices\n",
        "\n",
        "-   Covariance matrices are positive-definite, semi-definite if\n",
        "    degenerate\n",
        "-   Key property of Gaussian random variables:\n",
        "    -   $X \\sim N(\\mu, \\Sigma)$ for\n",
        "        $\\mu \\in \\mathbb{R}^N, \\Sigma \\in \\mathbb{R}^{N \\times N}$\n",
        "    -   $X = \\mu + A Z$ for $Z \\sim N(0_N, I_N)$ where $A A^T = \\Sigma$\n",
        "-   That is, $A$ is the Cholesky decomposition of the covariance matrix\n",
        "\n",
        "## Matrices as Linear Transformations\n",
        "\n",
        "-   Recall: for $x \\in \\mathbb{R}^N$ we should think of a $f(x) = A x$\n",
        "    for $A\\in\\mathbb{R}^{M \\times N}$ as a linear transformation from\n",
        "    $\\mathbb{R}^N$ to $\\mathbb{R}^M$\n",
        "    -   Definition of Linear: $f(a x_1 + b x_2) = a f(x_1) + b f(x_2)$\n",
        "        for scalar $a,b$\n",
        "-   Similarly, the $y = f(x) = A x$ then $f^{-1}(y) = A^{-1} y$ goes\n",
        "    from $\\mathbb{R}^M$ to $\\mathbb{R}^N$\n",
        "    -   If the matrix is square and invertible, we can go back and forth\n",
        "        without losing information (i.e., bijective). Otherwise we may\n",
        "        be projected onto a lower-dimensional “manifold”.\n",
        "\n",
        "## Norms and Linear Transformations\n",
        "\n",
        "-   The vector norm $||x||_2$ is an important feature in many\n",
        "    applications\n",
        "\n",
        "    -   Hence $||f(x)||_2 = ||A x||_2$ frequently comes up in\n",
        "        Quantitative Economics and Datascience\n",
        "    -   e.g. linear regression is written as minimizing a vector norm\n",
        "\n",
        "    $$\n",
        "    \\min_{\\beta} ||y - X \\beta||_2\n",
        "    $$\n",
        "\n",
        "-   Matrix structure or decompositions of $A$ help us better understand\n",
        "    the $f(x)$ mapping\n",
        "\n",
        "# Orthogonal Matrices\n",
        "\n",
        "## Orthogonal Matrices — Definition\n",
        "\n",
        "-   A square matrix $Q$ is **orthogonal** if $Q^\\top Q = Q Q^\\top = I$\n",
        "    (equivalently, $Q^{-1} = Q^\\top$).\n",
        "    -   Columns are orthonormal: if\n",
        "        $Q = \\begin{bmatrix} q_1 \\; | \\; \\ldots \\; | \\; q_N \\end{bmatrix}$\n",
        "        then\n",
        "        -   $q_i \\cdot q_j = 0$ for $i \\neq j$  \n",
        "        -   $q_i \\cdot q_i = 1$.\n",
        "    -   Geometric action: combination of **rotation** and/or\n",
        "        **reflection**\n",
        "    -   Inverse action: if $y = Q x$, then $x = Q^\\top y$.\n",
        "-   Example (reflection):\n",
        "    $Q = \\begin{bmatrix}1 & 0 \\\\ 0 & -1\\end{bmatrix}$ flips the second\n",
        "    component while preserving $\\|\\cdot\\|_2$.\n",
        "\n",
        "## Orthogonal Matrices — Preserving Norms\n",
        "\n",
        "-   Rotation and reflection preserve lengths\n",
        "-   For all $x \\in \\mathbb{R}^N$, $\\|Q x\\|_2 = \\|x\\|_2$.\n",
        "-   Proof:\n",
        "    $\\|Qx\\|_2^2 = (Qx) \\cdot (Qx) = (Q x)^\\top Q x  = x^\\top Q^\\top Q x = x^\\top x = \\|x\\|_2^2$.\n",
        "\n",
        "## Orthogonal Matrices — Preserving Angles\n",
        "\n",
        "-   For any $x_1, x_2 \\in \\mathbb{R}^N$,\n",
        "    $(Q x_1) \\cdot (Q x_2) = x_1 \\cdot x_2$.\n",
        "-   Proof:\n",
        "    $(Q x_1) \\cdot (Q x_2) = (Q x_1)^\\top (Q x_2) = x_1^\\top Q^\\top Q x_2 = x_1^\\top x_2 = x_1 \\cdot x_2$.\n",
        "-   Orthogonal maps preserve inner products/“angles”\n",
        "    -   e.g., rotate both the $x_1$ and $x_2$ exactly the same amount.\n",
        "    -   Useful in many applications such as Principal Component Analysis\n",
        "        (PCA) and cosine similarity (which is used in NLP embeddings).\n",
        "\n",
        "# Eigenvalues and Eigenvectors\n",
        "\n",
        "## Eigenvalues and Eigenvectors\n",
        "\n",
        "-   For a square $A$, an eigenvector $x$ and eigenvalue $\\lambda$\n",
        "    satisfy $$\n",
        "    A x = \\lambda x\n",
        "    $$\n",
        "\n",
        "-   $A\\in\\mathbb{R}^{N\\times N}$ has $N$ eigenvalue/eigenvector pairs,\n",
        "    possible multiplicity of $\\lambda$\n",
        "\n",
        "-   Intuition: $x$ is a direction $A x \\propto x$ and $\\lambda$ says how\n",
        "    much it “stretches”\n",
        "\n",
        "## Properties of Eigenvalues and Eigenvectors\n",
        "\n",
        "-   For any eigenvector $x$ and scalar $c$ then $c x \\propto A x$ as\n",
        "    well\n",
        "-   Symmetric matrices have real eigenvalues and orthogonal\n",
        "    eigenvectors. i.e. $x_1 \\cdot x_2 = 0$ for $x_1 \\neq x_2$\n",
        "    eigenvectors. Complex in general\n",
        "-   Singular if and only if it has an eigenvalue of zero\n",
        "-   Positive (semi)definite if and only if all eigenvalues are strictly\n",
        "    (weakly) positive\n",
        "-   Diagonal matrix has eigenvalues as its diagonal\n",
        "-   Triangular matrix has eigenvalues as its diagonal\n",
        "\n",
        "## Positive Definite and Eigenvalues\n",
        "\n",
        "You cannot check $x^T A x > 0$ for all $x$. Check if “stretching” is\n",
        "positive"
      ],
      "id": "012f6aa3-21b6-42b1-a27b-9baf120c1516"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.23606798  4.23606798]\n",
            "pos-def? False\n",
            "pos-semi-def? False"
          ]
        }
      ],
      "source": [
        "A = np.array([[3, 2],\n",
        "              [2, 1]])\n",
        "# A_eigs = np.real(eigvals(A)) # symmetric matrices have real eigenvalues\n",
        "A_eigs = eigvalsh(A) # specialized for symmetric/hermitian matrices\n",
        "print(A_eigs)\n",
        "is_positive_definite = np.all(A_eigs > 0)\n",
        "is_positive_semi_definite = np.all(A_eigs >= 0) # or eigvals(A) >= -eps\n",
        "print(f\"pos-def? {is_positive_definite}\")\n",
        "print(f\"pos-semi-def? {is_positive_semi_definite}\")"
      ],
      "id": "aeb9df10"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Positive Semi-Definite Matrices **May** Have a Zero Eigenvalue\n",
        "\n",
        "The simplest positive-semi-definite (but not posdef) matrix is"
      ],
      "id": "c8fd4db4-a31a-4655-b488-eecace64b4dd"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1.]\n",
            "pos-def? False\n",
            "pos-semi-def? True"
          ]
        }
      ],
      "source": [
        "A_eigs = eigvalsh(np.array([[1, 0],\n",
        "                            [0, 0]]))\n",
        "print(A_eigs)\n",
        "is_positive_definite = np.all(A_eigs > 0)\n",
        "is_positive_semi_definite = np.all(A_eigs >= 0) # or eigvals(A) >= -eps\n",
        "print(f\"pos-def? {is_positive_definite}\")\n",
        "print(f\"pos-semi-def? {is_positive_semi_definite}\")"
      ],
      "id": "f3c121d9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Eigen Decomposition (General)\n",
        "\n",
        "-   For real square matrix $A$ **we may be able to** take the\n",
        "    eigenvectors as columns of $Q$ (eigenvectors) and a diagonal matrix\n",
        "    $\\Lambda$ of the eigenvalues such that $$\n",
        "    A = Q \\, \\Lambda \\, Q^{-1}.\n",
        "    $$\n",
        "    -   $\\Lambda = \\operatorname{diag}(\\lambda_1,\\ldots,\\lambda_n)$\n",
        "        collects the eigenvalues (zeros allowed) into a diagonal matrix.\n",
        "    -   The eigenvectors and eigenvalues are complex in general\n",
        "-   These do not always exist. If so, you will hear the matrix is\n",
        "    **diagonalizable**.\n",
        "    -   Essentially, this is a question of whether the eigenvectors form\n",
        "        an $N$ dimensional basis and hence a $Q$ that is invertible.\n",
        "-   Alternatively, **every** real matrix admits a Singular Value\n",
        "    Decomposition (SVD)\n",
        "\n",
        "## Eigen Decomposition (Symmetric Case)\n",
        "\n",
        "-   A special case of this comes from the spectral theorem, which will\n",
        "    sometimes be referred to as a **spectral decomposition**\n",
        "-   For **any** real symmetric matrix $A$ (singular or not) there exist\n",
        "    an orthogonal $Q$ and a real diagonal $\\Lambda$ such that $$\n",
        "    A = Q \\, \\Lambda \\, Q^{\\top}.\n",
        "    $$\n",
        "    -   Orthogonal $Q^{\\top} Q = Q Q^{\\top} = I$ (so $Q^{-1}=Q^{\\top}$);\n",
        "        the columns of $Q$ form an **orthonormal basis** of\n",
        "        eigenvectors.\n",
        "    -   $\\Lambda = \\operatorname{diag}(\\lambda_1,\\ldots,\\lambda_n)$\n",
        "        collects the **real** eigenvalues (zeros allowed) into a\n",
        "        diagonal matrix. Complex in general for non-symmetric matrices,\n",
        "        but real for symmetric $A$.\n",
        "-   Geometric interpretation: apply $Q^{\\top}$ (rotation/reflection) to\n",
        "    rotate into the basis, scale by $\\Lambda$, then apply $Q$ to\n",
        "    rotate/reflect back.\n",
        "\n",
        "## Eigendecompositions and Matrix Powers\n",
        "\n",
        "-   Can be used to find $A^t$ for large $t$ (e.g. for Markov chains)\n",
        "    -   $P^t$, i.e. $P \\cdot P \\cdot \\ldots \\cdot P$ for $t$ times\n",
        "    -   $P = Q \\Lambda Q^{-1}$ then $P^t = Q \\Lambda^t Q^{-1}$ where\n",
        "        $\\Lambda^t$ is just the pointwise power\n",
        "-   Related tools such as SVD can help with dimensionality reduction\n",
        "\n",
        "## Spectral/Eigendecomposition of Symmetric Matrix Example"
      ],
      "id": "4386e5b9-76b2-4c37-bac8-36334dbaae03"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eigenvectors are column-by-column in Q =\n",
            "[[-0.85065081 -0.52573111]\n",
            " [ 0.52573111 -0.85065081]]\n",
            "Q^T Q =\n",
            "[[1.0000000e+00 1.2127222e-17]\n",
            " [1.2127222e-17 1.0000000e+00]]\n",
            "Q^-1 == Q^T? True\n",
            "eigenvalues are in Lambda = [1.38196601+0.j 3.61803399+0.j]\n",
            "Q Lambda Q^T =\n",
            "[[2. 1.]\n",
            " [1. 3.]]"
          ]
        }
      ],
      "source": [
        "A = np.array([[2, 1],\n",
        "              [1, 3]])\n",
        "Lambda, Q = eig(A)\n",
        "print(f\"eigenvectors are column-by-column in Q =\\n{Q}\")\n",
        "print(f\"Q^T Q =\\n{Q.T @ Q}\") # should be I\n",
        "print(f\"Q^-1 == Q^T? {np.allclose(np.linalg.inv(Q), Q.T)}\") # should be True\n",
        "print(f\"eigenvalues are in Lambda = {Lambda}\")\n",
        "print(f\"Q Lambda Q^T =\\n{Q @ np.diag(np.real(Lambda)) @ Q.T}\")"
      ],
      "id": "094fdf05"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spectral/Eigendecomposition of an Asymmetric Matrix Example"
      ],
      "id": "9d7468f4-b85f-4a30-841b-80d3d6a996fe"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eigenvectors are column-by-column in Q =\n",
            "[[ 0.80689822 -0.34372377]\n",
            " [ 0.59069049  0.9390708 ]]\n",
            "Q^-1 == Q^T? False\n",
            "eigenvalues are in Lambda = [5.73205081+0.j 2.26794919+0.j]\n",
            "Q Lambda inv(Q) =\n",
            "[[5. 1.]\n",
            " [2. 3.]]"
          ]
        }
      ],
      "source": [
        "A = np.array([[5, 1],\n",
        "              [2, 3]])\n",
        "Lambda, Q = eig(A)\n",
        "print(f\"eigenvectors are column-by-column in Q =\\n{Q}\")\n",
        "print(f\"Q^-1 == Q^T? {np.allclose(np.linalg.inv(Q), Q.T)}\") # should be False\n",
        "print(f\"eigenvalues are in Lambda = {Lambda}\")\n",
        "print(f\"Q Lambda inv(Q) =\\n{Q @ np.diag(np.real(Lambda)) @ np.linalg.inv(Q)}\")"
      ],
      "id": "46dbb2ed"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spectral Radius is Maximum Absolute Eigenvalue\n",
        "\n",
        "-   If any $\\lambda \\in \\Lambda$ are $> 1$ can see this would explode\n",
        "-   Useful for seeing if iteration $x_{t+1} = A x_t$ from a $x_0$\n",
        "    explodes\n",
        "-   The **spectral radius** of matrix $A$ is\n",
        "\n",
        "$$\n",
        "  \\rho(A) = \\max_{\\lambda \\in \\Lambda}|\\lambda|\n",
        "  $$\n",
        "\n",
        "## Recall Condition Numbers\n",
        "\n",
        "-   The condition number of the matrix $A$ is the ratio of the largest\n",
        "    and smallest eigenvalues\n",
        "    -   $\\kappa(A) = \\left|\\frac{\\lambda_{\\text{max}}}{\\lambda_{\\text{min}}}\\right|$\n",
        "        for $\\lambda$ the eigenvalues of $A$.\n",
        "    -   Sense of relative dispersion and consistent scaling\n",
        "-   For example, using the fact that the triangular matrix has\n",
        "    eigenvalues on the diagonal $$\n",
        "    A = \\begin{bmatrix}\n",
        "    1 & 2 \\\\\n",
        "    0 & 0.001\n",
        "    \\end{bmatrix}\n",
        "    $$\n",
        "    -   $\\kappa(A) = 1/0.001 = 1000$ and $\\rho(A) = 1$.\n",
        "-   Or, the best we can do is $\\kappa(A) = 1$ for the identity (or\n",
        "    triangular with 1’s on diagonal) $$\n",
        "    A = \\begin{bmatrix}\n",
        "    1 & 0 \\\\\n",
        "    0 & 1\n",
        "    \\end{bmatrix}\n",
        "    $$"
      ],
      "id": "1bd3127d-94c7-4945-8070-37167be8cf8b"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/home/runner/work/grad_econ_datascience/grad_econ_datascience/.venv/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  }
}