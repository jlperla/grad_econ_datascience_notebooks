{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Foundations of Numerical Linear Algebra\n",
        "\n",
        "Graduate Quantitative Economics and Datascience\n",
        "\n",
        "Jesse Perla (University of British Columbia)\n",
        "\n",
        "## Going Beyond “`reg y x, robust`”\n",
        "\n",
        "-   Data science, econometrics, and macroeconomics are built on linear\n",
        "    algebra.\n",
        "-   Numerical linear algebra has all sorts of pitfalls, which become\n",
        "    more critical as we scale up to larger problems.\n",
        "-   Speed differences in choosing better algorithms can be orders of\n",
        "    magnitude.\n",
        "-   Crucial to know what goes on under-the-hood in Stata/R/python\n",
        "    packages for applied work, even if you don’t implement it yourself.\n",
        "\n",
        "## Extra Materials\n",
        "\n",
        "-   Material related to: [QuantEcon\n",
        "    Python](https://python.quantecon.org/linear_algebra.html),\n",
        "    [QuantEcon Data\n",
        "    Science](https://datascience.quantecon.org/scientific/applied_linalg.html),\n",
        "    [Intro Quantitative Economics with\n",
        "    Python](https://intro.quantecon.org/)\n",
        "-   **Self-study and Optional Materials:**\n",
        "    -   [Basics of linear algebra, matrices, norms, and linear\n",
        "        independence](https://python.quantecon.org/linear_algebra.html)\n",
        "    -   [Numerical\n",
        "        optimization](https://datascience.quantecon.org/scientific/optimization.html)\n",
        "    -   [Systems of\n",
        "        Equations](https://python.quantecon.org/linear_algebra.html#solving-systems-of-equations)\n",
        "    -   [Eigenvectors and\n",
        "        eigenvalues](https://python.quantecon.org/linear_algebra.html#eigenvalues-and-eigenvectors)\n",
        "    -   [Downloading and manipulating data in\n",
        "        Python](https://intro.quantecon.org/long_run_growth.html) and\n",
        "        [here](https://intro.quantecon.org/business_cycle.html)\n",
        "    -   [Introductory material on linear\n",
        "        algebra](https://intro.quantecon.org/linear_equations.html) and\n",
        "        [more](https://datascience.quantecon.org/scientific/applied_linalg.html)\n",
        "    -   [Matrix decompositions and other\n",
        "        topics](https://python.quantecon.org/linear_algebra.html#further-topics)\n",
        "\n",
        "## Packages\n",
        "\n",
        "This section uses the following packages:"
      ],
      "id": "54ee8ab8-a9a8-4298-a5b3-ddc3551ea0be"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from numpy.linalg import cond, matrix_rank, norm\n",
        "from scipy.linalg import inv, solve, det, eig, lu, eigvals\n",
        "from scipy.linalg import solve_triangular, eigvalsh, cholesky"
      ],
      "id": "0d821dd1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Computational Complexity\n",
        "\n",
        "**Big-O Notation**\n",
        "\n",
        "For a function $f(N)$ and a positive constant $C$, we say $f(N)$ is\n",
        "$O(g(N))$, if there exist positive constants $C$ and $N_0$ such that:\n",
        "\n",
        "$$\n",
        "0 \\leq f(N) \\leq C \\cdot g(N) \\quad \\text{for all } N \\geq N_0\n",
        "$$\n",
        "\n",
        "-   Often crucial to know how problems scale asymptotically (as\n",
        "    $N\\to\\infty$)\n",
        "-   Caution! This is only an asymptotic limit, and can be misleading for\n",
        "    small $N$\n",
        "    -   $f_1(N) = N^3 + N$ is $O(N^3)$\n",
        "    -   $f_2(N) = 1000 N^2 + 3 N$ is $O(N^2)$\n",
        "    -   For roughly $N>1000$ use $f_2$ algorithm, otherwise $f_1$\n",
        "\n",
        "## Examples of Computational Complexity\n",
        "\n",
        "-   Simple examples:\n",
        "    -   $x \\cdot y = \\sum_{n=1}^N x_n y_n$ is $O(N)$ since it requires\n",
        "        $N$ multiplications and additions\n",
        "    -   $A x$ for $A\\in\\mathbb{R}^{N\\times N},x\\in\\mathbb{R}^N$ is\n",
        "        $O(N^2)$ since it requires $N$ dot products, each $O(N)$\n",
        "\n",
        "## Numerical Precision\n",
        "\n",
        "**Machine Epsilon**\n",
        "\n",
        "For a given datatype, $\\epsilon$ is defined as\n",
        "$\\epsilon = \\min_{\\delta > 0} \\left\\{ \\delta : 1 + \\delta > 1 \\right\\}$\n",
        "\n",
        "-   Computers have finite precision. 64-bit typical, but 32-bit on GPUs"
      ],
      "id": "138b4b77-49ac-48cd-8218-9ea5b4195739"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "machine epsilon for float64 = 2.220446049250313e-16\n",
            "1 + eps/2 == 1? True\n",
            "machine epsilon for float32 = 1.1920928955078125e-07"
          ]
        }
      ],
      "source": [
        "print(f\"machine epsilon for float64 = {np.finfo(float).eps}\")\n",
        "print(f\"1 + eps/2 == 1? {1.0 + 1.1e-16 == 1.0}\")\n",
        "print(f\"machine epsilon for float32 = {np.finfo(np.float32).eps}\")"
      ],
      "id": "3f35307b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basic Linear Algebra\n",
        "\n",
        "## Norms\n",
        "\n",
        "-   Common measure of size is the Euclidean norm, or $L^2$ norm for\n",
        "    $x \\in \\mathbb{R}^2$\n",
        "-   Complexity is $O(N)$, square $N$ times then $N$ additions $$\n",
        "    ||x||_2 = \\sqrt{\\sum_{n=1}^N x_n^2}\n",
        "    $$"
      ],
      "id": "25f335dc-eb1f-463c-b89c-3c039ac2ce2a"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.7416573867739413\n",
            "3.7416573867739413\n",
            "3.7416573867739413\n",
            "||x||_2^2 = 14.0 = 14 = 14"
          ]
        }
      ],
      "source": [
        "x = np.array([1, 2, 3]) # Calculating different ways (in order of preference)\n",
        "print(np.sqrt(sum(xval**2 for xval in x))) # manual with comprehensions\n",
        "print(np.sqrt(np.sum(np.square(x)))) # broadcasts\n",
        "print(norm(x)) # built-in to numpy norm(x, ord=2) alternatively\n",
        "print(f\"||x||_2^2 = {norm(x)**2} = {x.T @ x} = {np.dot(x, x)}\")"
      ],
      "id": "f773e0a1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solving Systems of Equations\n",
        "\n",
        "-   Solving $A x = b$ for $x$ is equivalent $A^{-1} A x = A^{-1} b$\n",
        "-   Then since $A^{-1}A = I$, and $I x = x$, we have $x = A^{-1} b$\n",
        "-   Careful since matrix algebra is not commutative!"
      ],
      "id": "f5047451-3837-4f27-a258-baad6e8c7bf9"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([-1.,  1.])"
            ]
          }
        }
      ],
      "source": [
        "A = np.array([[0, 2],\n",
        "              [3, 4]]) # or ((0, 2), (3, 4))\n",
        "b = np.array([2,1])  # Column vector\n",
        "x = solve(A, b)  # Solve Ax = b for x\n",
        "x"
      ],
      "id": "267e1a0e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using the Inverse Directly\n",
        "\n",
        "-   Can replace the `solve` with a calculation of an inverse\n",
        "-   But it can be slower or less accurate than solving the system\n",
        "    directly"
      ],
      "id": "dbe5b423-f3e1-404e-9b69-e273f52aa815"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([-1.,  1.])"
            ]
          }
        }
      ],
      "source": [
        "A_inv = inv(A)\n",
        "A_inv @ b # i.e, A^{-1} * b"
      ],
      "id": "0d39f62a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Combinations\n",
        "\n",
        "We can think of solving a system as finding the linear combination of\n",
        "columns of $A$ that equal $b$"
      ],
      "id": "c0381c80-0208-42c8-bca5-3bbd6eb6ddea"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b = [2 1], b_star = [2. 1.]"
          ]
        }
      ],
      "source": [
        "b_star = x[0] * A[:, 0] + x[1] * A[:, 1] # using x solution\n",
        "print(f\"b = {b}, b_star = {b_star}\")"
      ],
      "id": "878a530d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Column Space and Rank\n",
        "\n",
        "-   The column space of a matrix represents all possible linear\n",
        "    combinations of its columns.\n",
        "-   It forms a basis for the space of solutions when solving systems of\n",
        "    linear equations represented by the matrix\n",
        "-   The rank of a matrix is the dimension of its column space"
      ],
      "id": "a2f0bde5-89b9-4915-88d7-8bf1b55d8ffc"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "np.int64(2)"
            ]
          }
        }
      ],
      "source": [
        "A = np.array([[0, 2],\n",
        "              [3, 4]])\n",
        "matrix_rank(A)"
      ],
      "id": "8b674bc0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hence, can solve $A x = b$ for any $b \\in \\mathbb{R}^2$ since the column\n",
        "space is the entire space $\\mathbb{R}^2$\n",
        "\n",
        "## Singular Matrices\n",
        "\n",
        "On the other hand, note"
      ],
      "id": "2e3b4cb0-64a0-4af4-9884-245b5cb6137c"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "np.int64(1)"
            ]
          }
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 4]])\n",
        "matrix_rank(A)"
      ],
      "id": "6c1b8c91"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So we can only solve $A x = b$ for\n",
        "$b \\propto \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} \\propto \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$\n",
        "\n",
        "## Checking Singularity"
      ],
      "id": "eaae938b-3b67-4531-9dbd-96b574dc3564"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "Matrix is singular (non-invertible)."
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 4]])\n",
        "# An (expensive) way to check if A is singular is if det(A) = 0\n",
        "print(det(A) == 0.0)\n",
        "print(matrix_rank(A) != A.shape[0]) # or check rank\n",
        "# Check before inverting or use exceptions\n",
        "try:\n",
        "    inv(A)\n",
        "    print(\"Matrix is not singular (invertible).\")\n",
        "except np.linalg.LinAlgError:\n",
        "    print(\"Matrix is singular (non-invertible).\")"
      ],
      "id": "a27b22e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Determinant is Not Scale Invariant\n",
        "\n",
        "-   Reminder: numerical precision in calculations makes it hard to\n",
        "    compare to zero\n",
        "-   The determinate is useful but depends on the scale of the matrix\n",
        "-   A more robust alternative is the condition number (more next\n",
        "    lecture)"
      ],
      "id": "a8aea2ea-d1ac-4080-af3f-2a1881ad4e3c"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "det(A)=-1e-08, det(K*A)=-100\n",
            "cond(A)=1e+09, cond(K*A)=1e+09,\n",
            "det(inv(A))=-1e+08, cond(inv(A))=1e+09"
          ]
        }
      ],
      "source": [
        "eps, K = 1e-8, 100000\n",
        "A = np.array([[1, 2],\n",
        "              [1 + eps, 2 + eps]]) # not 1 + eps, 2(1+eps)!\n",
        "print(f\"det(A)={det(A):.5g}, det(K*A)={det(K*A):.5g}\")\n",
        "print(f\"cond(A)={cond(A):.5g}, cond(K*A)={cond(K*A):.5g},\")\n",
        "print(f\"det(inv(A))={det(inv(A)):.5g}, cond(inv(A))={cond(inv(A)):.5g}\")"
      ],
      "id": "d36fa479"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpreting Condition Numbers\n",
        "\n",
        "-   The condition number of the matrix $A$ is\n",
        "    $\\kappa(A) = ||A|| \\cdot ||A^{-1}||$, which can be shown in terms of\n",
        "    ratio of the largest and smallest eigenvalues\n",
        "    -   $\\kappa(A) = \\left| \\frac{\\lambda_{\\text{max}}}{\\lambda_{\\text{min}}} \\right|$\n",
        "        for $\\lambda$ the eigenvalues of $A$. More soon!\n",
        "-   Crude intuition: for machine epsilon $\\epsilon_{\\text{mach}}$ when\n",
        "    calculating some $x$\n",
        "    -   The relative error, $||x - x_{\\text{approx}}||/||x||$ is roughly\n",
        "        $\\kappa(A) \\cdot \\epsilon_{\\text{mach}}$\n",
        "    -   Solving $A x = b$ when $\\epsilon_{\\text{mach}} = 1e^{-16}$ it\n",
        "        amplifies errors in $b$, etc.\n",
        "    -   if $\\kappa(A) \\approx 1e^{16}$ errors amplified so the scale of\n",
        "        100% relative error\n",
        "\n",
        "## Rules of Thumb\n",
        "\n",
        "-   Rule of thumb for standard floating points where\n",
        "    $\\epsilon_{\\text{mach}} = 1e^{-16}$:\n",
        "    -   $\\kappa(A) \\approx 1$ well-conditioned\n",
        "    -   $\\kappa(A) < 100$ fairly well-conditioned\n",
        "    -   $\\kappa(A) < 1e^{5}$ moderately ill-conditioned. Take care\n",
        "    -   $\\kappa(A) < 1e^{8}$ ill-conditioned and might introduce\n",
        "        significant errors, especially in algorithms which repeatedly\n",
        "        use the same calculations\n",
        "    -   $\\kappa(A) > 1e^{8}$ very ill-conditioned and likely to\n",
        "        introduce significant errors\n",
        "-   Choose solution algorithms based on “numerical stability” and\n",
        "    “conditioning” when worried\n",
        "-   Much more extreme with 32-bit floats such as when using GPUs.\n",
        "\n",
        "# Solving Linear Systems of Equations\n",
        "\n",
        "## Solving Systems with Multiple RHS\n",
        "\n",
        "-   Inverse is nice because you can reuse the $A^{-1}$ to solve\n",
        "    $A x = b$ for many $b$\n",
        "-   However, you can do this with `solve` as well\n",
        "-   Or can reuse LU factorizations (discussed next)"
      ],
      "id": "5e546398-a1c4-473f-820b-7deb626998f4"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.         -1.33333333]\n",
            " [ 1.          1.5       ]]\n",
            "Checking: A*[-1.  1.] = [2. 1.] = [2 1], column of B"
          ]
        }
      ],
      "source": [
        "A = np.array([[0, 2],\n",
        "              [3, 4]])\n",
        "B = np.array([[2, 3],\n",
        "              [1, 2]])  # [2,1] and [3,2] as columns\n",
        "# or: B = np.column_stack([np.array([2, 1]),np.array([3,2])])\n",
        "X = solve(A, B)  # Solve AX = B for X\n",
        "print(X)\n",
        "print(f\"Checking: A*{X[:,0]} = {A@X[:, 0]} = {B[:,0]}, column of B\")"
      ],
      "id": "12c6aea9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LU(P) Decompositions\n",
        "\n",
        "-   We can “factor” any square $A$ into $P A = L U$ for triangular $L$\n",
        "    and $U$. Invertible can have $A = L U$, called the LU decomposition.\n",
        "    “P” is for partial-pivoting\n",
        "-   Singular matrices may not have full-rank $L$ or $U$ matrices"
      ],
      "id": "19f6da19-842a-49b7-a9cf-1901ac544196"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L*U =\n",
            "[[2. 4.]\n",
            " [1. 2.]]\n",
            "P*A =\n",
            "[[2. 4.]\n",
            " [1. 2.]]"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 4]])\n",
        "P, L, U = lu(A)\n",
        "print(f\"L*U =\\n{L @ U}\")\n",
        "print(f\"P*A =\\n{P @ A}\")"
      ],
      "id": "e9c53c93"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## P, U, and L\n",
        "\n",
        "The $P$ matrix is a permutation matrix of “pivots” the others are\n",
        "triangular"
      ],
      "id": "5723f795-4664-4368-84f7-13fd8a48199b"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P =\n",
            "[[0. 1.]\n",
            " [1. 0.]]\n",
            "L =\n",
            "[[1.  0. ]\n",
            " [0.5 1. ]]\n",
            "U =\n",
            "[[2. 4.]\n",
            " [0. 0.]]"
          ]
        }
      ],
      "source": [
        "print(f\"P =\\n{P}\")\n",
        "print(f\"L =\\n{L}\")\n",
        "print(f\"U =\\n{U}\")"
      ],
      "id": "91897527"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LU Decompositions and Systems of Equations\n",
        "\n",
        "-   Pivoting is typically implied when talking about “LU”\n",
        "-   Used in the default `solve` algorithm (without more structure)\n",
        "-   Solving systems of equations with triangular matrices: for\n",
        "    $A x = L U x = b$\n",
        "    1.  Define $y = U x$\n",
        "    2.  Solve $L y = b$ for $y$ and $U x = y$ for $x$\n",
        "-   Since both are triangular, process is $O(N^2)$ (but LU itself\n",
        "    $O(N^3)$)\n",
        "-   Could be used to find `inv`\n",
        "    -   $A = L U$ then $A A^{-1} = I = L U A^{-1} = I$\n",
        "    -   Solve for $Y$ in $L Y = I$, then solve $U A^{-1} = Y$\n",
        "-   Tight connection to textbook Gaussian elimination (including\n",
        "    pivoting)\n",
        "\n",
        "## LU for Non-Singular Matrices"
      ],
      "id": "4e367a91-5992-41fa-bab9-43d672c92cb0"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L*U =\n",
            "[[3. 4.]\n",
            " [1. 2.]]\n",
            "P*A =\n",
            "[[3. 4.]\n",
            " [1. 2.]]"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [3, 4]])\n",
        "P, L, U = lu(A)\n",
        "print(f\"L*U =\\n{L @ U}\")\n",
        "print(f\"P*A =\\n{P @ A}\")"
      ],
      "id": "3d6a62fb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## L, U, P"
      ],
      "id": "9f9051b8-5c45-490a-a786-5d673f71b6c8"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P =\n",
            "[[0. 1.]\n",
            " [1. 0.]]\n",
            "L =\n",
            "[[1.         0.        ]\n",
            " [0.33333333 1.        ]]\n",
            "U =\n",
            "[[3.         4.        ]\n",
            " [0.         0.66666667]]"
          ]
        }
      ],
      "source": [
        "print(f\"P =\\n{P}\")\n",
        "print(f\"L =\\n{L}\")\n",
        "print(f\"U =\\n{U}\")"
      ],
      "id": "8ee16847"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backwards Substitution Example\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "U x &= b\\\\\n",
        "U &\\equiv \\begin{bmatrix}\n",
        "3 & 1 \\\\\n",
        "0 & 2 \\\\\n",
        "\\end{bmatrix}, \\quad b = \\begin{bmatrix}\n",
        "7 \\\\\n",
        "2 \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Solving bottom row for $x_2$\n",
        "\n",
        "$$\n",
        "2 x_2 = 2,\\quad x_2 = 1\n",
        "$$\n",
        "\n",
        "Move up a row, solving for $x_1$, substituting for $x_2$\n",
        "\n",
        "$$\n",
        "3 x_1 + 1 x_2 = 7,\\quad 3 x_1 + 1 \\times 1 =  7,\\quad x_1 = 2\n",
        "$$\n",
        "\n",
        "Generalizes to many rows. For $L$ it is “forward substitution”\n",
        "\n",
        "## Use Triangular Structure if Possible\n",
        "\n",
        "-   Triangular matrices of size $N$ can be solved with back substitution\n",
        "    in $O(N^2)$\n",
        "-   Is $O(N^2)$ good or bad? Beats, $O(N^3)$ typical of general methods"
      ],
      "id": "715484c2-6728-468a-b597-7073e24a9575"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([2., 1.])"
            ]
          }
        }
      ],
      "source": [
        "U = np.array([[3, 1],\n",
        "              [0, 2]])\n",
        "b = np.array([7,2])\n",
        "solve(U,b) # works, but internally does an LU which is O(N^3)\n",
        "solve_triangular(U, b, lower=False) # fast O(N^2)"
      ],
      "id": "78cd7b52"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Symmetric and Positive Definite Matrices\n",
        "\n",
        "## Symmetric Matrix Structure\n",
        "\n",
        "Another common matrix type are symmetric, $A = A^{T}$"
      ],
      "id": "8be12758-99fb-4871-8062-591b8abe35a6"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([-3.,  2.])"
            ]
          }
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 5]]) # also posdef, not singular\n",
        "b = np.array([1,4])\n",
        "# With scipy 1.11.3 check with scipy.linalg.issymmetric(A)\n",
        "solve(A, b, assume_a=\"sym\") # could also use \"pos\" since positive definite"
      ],
      "id": "307e3b46"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Positive Definite Matrices\n",
        "\n",
        "-   A symmetric matrix $A$ is positive definite if $x^T A x > 0$ for all\n",
        "    $x \\neq 0$\n",
        "-   Useful in many areas, such as covariance matrices. Example"
      ],
      "id": "ad8a6da1-15f0-464a-b39e-b46a533f08ff"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x^T A x = 5"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 5]])\n",
        "x = np.array([0, 1]) # can't really check for all x\n",
        "print(f\"x^T A x = {x.T @ A @ x}\")"
      ],
      "id": "4de5202a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Example of a symmetric matrix that is not positive definite"
      ],
      "id": "5e3f656e-bd6d-43aa-8cb9-363d1bb563e9"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x^T A x = 0"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 0]])\n",
        "print(f\"x^T A x = {x.T @ A @ x}\")  # one counterexample is enough"
      ],
      "id": "8506d14b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   We can check these with eigenvalues\n",
        "\n",
        "## Cholesky Decomposition\n",
        "\n",
        "-   For symmetric positive definite matrices: $L = U^T$’\n",
        "-   Called a Cholesky decomposition: $A = L L^T$ for a lower triangular\n",
        "    matrix $L$.\n",
        "-   Equivalently, could find $A = U^T U$ for an upper triangular matrix\n",
        "    $U$"
      ],
      "id": "c78cae78-53ac-4d4e-9832-6baf2d70bee3"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0.]\n",
            " [2. 1.]]\n",
            "L*L^T =\n",
            "[[1. 2.]\n",
            " [2. 5.]]"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 5]])\n",
        "L = cholesky(A, lower=True) # cholesky also defined for upper=True\n",
        "print(L)\n",
        "print(f\"L*L^T =\\n{L @ L.T}\")"
      ],
      "id": "bbde96f7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solving Positive Definite Systems"
      ],
      "id": "6b2a5042-61a8-4d5e-a90c-7b01c2110d3b"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-3.  2.]\n",
            "[-3.  2.]"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 5]])\n",
        "b = np.array([1, 4])\n",
        "print(solve(A, b, assume_a=\"pos\")) # uses cholesky internally\n",
        "\n",
        "L = cholesky(A, lower=True)\n",
        "y = solve_triangular(L, b, lower=True)\n",
        "x = solve_triangular(L.T, y, lower=False)\n",
        "print(x)"
      ],
      "id": "e600addc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cholesky for Covariance Matrices\n",
        "\n",
        "-   Covariance matrices are positive-definite, semi-definite if\n",
        "    degenerate\n",
        "-   Key property of Gaussian random variables:\n",
        "    -   $X \\sim N(\\mu, \\Sigma)$ for\n",
        "        $\\mu \\in \\mathbb{R}^N, \\Sigma \\in \\mathbb{R}^{N \\times N}$\n",
        "    -   $X = \\mu + A Z$ for $Z \\sim N(0_N, I_N)$ where $A A^T = \\Sigma$\n",
        "-   That is, $A$ is the Cholesky decomposition of the covariance matrix\n",
        "\n",
        "## Matrices as Linear Transformations\n",
        "\n",
        "-   Recall: for $x \\in \\mathbb{R}^N$ we should think of a $f(x) = A x$\n",
        "    for $A\\in\\mathbb{R}^{M \\times N}$ as a linear transformation from\n",
        "    $\\mathbb{R}^N$ to $\\mathbb{R}^M$\n",
        "    -   Definition of Linear: $f(a x_1 + b x_2) = a f(x_1) + b f(x_2)$\n",
        "        for scalar $a,b$\n",
        "-   Similarly, the $y = f(x) = A x$ then $f^{-1}(y) = A^{-1} y$ goes\n",
        "    from $\\mathbb{R}^M$ to $\\mathbb{R}^N$\n",
        "    -   If the matrix is square and invertible, we can go back and forth\n",
        "        without losing information (i.e., bijective). Otherwise we may\n",
        "        be projected onto a lower-dimensional “manifold”.\n",
        "\n",
        "## Norms and Linear Transformations\n",
        "\n",
        "-   The vector norm $||x||_2$ is an important feature in many\n",
        "    applications\n",
        "\n",
        "    -   Hence $||f(x)||_2 = ||A x||_2$ frequently comes up in\n",
        "        Quantitative Economics and Datascience\n",
        "    -   e.g. linear regression is written as minimizing a vector norm\n",
        "\n",
        "    $$\n",
        "    \\min_{\\beta} ||y - X \\beta||_2\n",
        "    $$\n",
        "\n",
        "-   Matrix structure or decompositions of $A$ help us better understand\n",
        "    the $f(x)$ mapping\n",
        "\n",
        "# Orthogonal Matrices\n",
        "\n",
        "## Orthogonal Matrices — Definition\n",
        "\n",
        "-   A square matrix $Q$ is **orthogonal** if $Q^\\top Q = Q Q^\\top = I$\n",
        "    (equivalently, $Q^{-1} = Q^\\top$).\n",
        "    -   Columns are orthonormal: if\n",
        "        $Q = \\begin{bmatrix} q_1 \\; | \\; \\ldots \\; | \\; q_N \\end{bmatrix}$\n",
        "        then\n",
        "        -   $q_i \\cdot q_j = 0$ for $i \\neq j$  \n",
        "        -   $q_i \\cdot q_i = 1$.\n",
        "    -   Geometric action: combination of **rotation** and/or\n",
        "        **reflection**\n",
        "    -   Inverse action: if $y = Q x$, then $x = Q^\\top y$.\n",
        "-   Example (reflection):\n",
        "    $Q = \\begin{bmatrix}1 & 0 \\\\ 0 & -1\\end{bmatrix}$ flips the second\n",
        "    component while preserving $\\|\\cdot\\|_2$.\n",
        "\n",
        "## Orthogonal Matrices — Preserving Norms\n",
        "\n",
        "-   Rotation and reflection preserve lengths\n",
        "-   For all $x \\in \\mathbb{R}^N$, $\\|Q x\\|_2 = \\|x\\|_2$.\n",
        "-   Proof:\n",
        "    $\\|Qx\\|_2^2 = (Qx) \\cdot (Qx) = (Q x)^\\top Q x  = x^\\top Q^\\top Q x = x^\\top x = \\|x\\|_2^2$.\n",
        "\n",
        "## Orthogonal Matrices — Preserving Angles\n",
        "\n",
        "-   For any $x_1, x_2 \\in \\mathbb{R}^N$,\n",
        "    $(Q x_1) \\cdot (Q x_2) = x_1 \\cdot x_2$.\n",
        "-   Proof:\n",
        "    $(Q x_1) \\cdot (Q x_2) = (Q x_1)^\\top (Q x_2) = x_1^\\top Q^\\top Q x_2 = x_1^\\top x_2 = x_1 \\cdot x_2$.\n",
        "-   Orthogonal maps preserve inner products/“angles”\n",
        "    -   e.g., rotate both the $x_1$ and $x_2$ exactly the same amount.\n",
        "    -   Useful in many applications such as Principal Component Analysis\n",
        "        (PCA) and cosine similarity (which is used in NLP embeddings).\n",
        "\n",
        "# Eigenvalues and Eigenvectors\n",
        "\n",
        "## Eigenvalues and Eigenvectors\n",
        "\n",
        "-   For a square $A$, an eigenvector $x$ and eigenvalue $\\lambda$\n",
        "    satisfy $$\n",
        "    A x = \\lambda x\n",
        "    $$\n",
        "\n",
        "-   $A\\in\\mathbb{R}^{N\\times N}$ has $N$ eigenvalue/eigenvector pairs,\n",
        "    possible multiplicity of $\\lambda$\n",
        "\n",
        "-   Intuition: $x$ is a direction $A x \\propto x$ and $\\lambda$ says how\n",
        "    much it “stretches”\n",
        "\n",
        "## Properties of Eigenvalues and Eigenvectors\n",
        "\n",
        "-   For any eigenvector $x$ and scalar $c$ then $c x \\propto A x$ as\n",
        "    well\n",
        "-   Symmetric matrices have real eigenvalues and orthogonal\n",
        "    eigenvectors. i.e. $x_1 \\cdot x_2 = 0$ for $x_1 \\neq x_2$\n",
        "    eigenvectors. Complex in general\n",
        "-   Singular if and only if it has an eigenvalue of zero\n",
        "-   Positive (semi)definite if and only if all eigenvalues are strictly\n",
        "    (weakly) positive\n",
        "-   Diagonal matrix has eigenvalues as its diagonal\n",
        "-   Triangular matrix has eigenvalues as its diagonal\n",
        "\n",
        "## Positive Definite and Eigenvalues\n",
        "\n",
        "You cannot check $x^T A x > 0$ for all $x$. Check if “stretching” is\n",
        "positive"
      ],
      "id": "029b5579-9177-47f9-94b0-c9760b5eca8c"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.23606798  4.23606798]\n",
            "pos-def? False\n",
            "pos-semi-def? False"
          ]
        }
      ],
      "source": [
        "A = np.array([[3, 2],\n",
        "              [2, 1]])\n",
        "# A_eigs = np.real(eigvals(A)) # symmetric matrices have real eigenvalues\n",
        "A_eigs = eigvalsh(A) # specialized for symmetric/hermitian matrices\n",
        "print(A_eigs)\n",
        "is_positive_definite = np.all(A_eigs > 0)\n",
        "is_positive_semi_definite = np.all(A_eigs >= 0) # or eigvals(A) >= -eps\n",
        "print(f\"pos-def? {is_positive_definite}\")\n",
        "print(f\"pos-semi-def? {is_positive_semi_definite}\")"
      ],
      "id": "bda002ce"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Positive Semi-Definite Matrices **May** Have a Zero Eigenvalue\n",
        "\n",
        "The simplest positive-semi-definite (but not posdef) matrix is"
      ],
      "id": "b3b19fb9-f2b9-4ee2-ac9c-326939b045a8"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1.]\n",
            "pos-def? False\n",
            "pos-semi-def? True"
          ]
        }
      ],
      "source": [
        "A_eigs = eigvalsh(np.array([[1, 0],\n",
        "                            [0, 0]]))\n",
        "print(A_eigs)\n",
        "is_positive_definite = np.all(A_eigs > 0)\n",
        "is_positive_semi_definite = np.all(A_eigs >= 0) # or eigvals(A) >= -eps\n",
        "print(f\"pos-def? {is_positive_definite}\")\n",
        "print(f\"pos-semi-def? {is_positive_semi_definite}\")"
      ],
      "id": "865e4fd5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Eigen Decomposition (General)\n",
        "\n",
        "-   For real square matrix $A$ **we may be able to** take the\n",
        "    eigenvectors as columns of $Q$ (eigenvectors) and a diagonal matrix\n",
        "    $\\Lambda$ of the eigenvalues such that $$\n",
        "    A = Q \\, \\Lambda \\, Q^{-1}.\n",
        "    $$\n",
        "    -   $\\Lambda = \\operatorname{diag}(\\lambda_1,\\ldots,\\lambda_n)$\n",
        "        collects the eigenvalues (zeros allowed) into a diagonal matrix.\n",
        "    -   The eigenvectors and eigenvalues are complex in general\n",
        "-   These do not always exist. If so, you will hear the matrix is\n",
        "    **diagonalizable**.\n",
        "    -   Essentially, this is a question of whether the eigenvectors form\n",
        "        an $N$ dimensional basis and hence a $Q$ that is invertible.\n",
        "-   Alternatively, **every** real matrix admits a Singular Value\n",
        "    Decomposition (SVD)\n",
        "\n",
        "## Eigen Decomposition (Symmetric Case)\n",
        "\n",
        "-   A special case of this comes from the spectral theorem, which will\n",
        "    sometimes be referred to as a **spectral decomposition**\n",
        "-   For **any** real symmetric matrix $A$ (singular or not) there exist\n",
        "    an orthogonal $Q$ and a real diagonal $\\Lambda$ such that $$\n",
        "    A = Q \\, \\Lambda \\, Q^{\\top}.\n",
        "    $$\n",
        "    -   Orthogonal $Q^{\\top} Q = Q Q^{\\top} = I$ (so $Q^{-1}=Q^{\\top}$);\n",
        "        the columns of $Q$ form an **orthonormal basis** of\n",
        "        eigenvectors.\n",
        "    -   $\\Lambda = \\operatorname{diag}(\\lambda_1,\\ldots,\\lambda_n)$\n",
        "        collects the **real** eigenvalues (zeros allowed) into a\n",
        "        diagonal matrix. Complex in general for non-symmetric matrices,\n",
        "        but real for symmetric $A$.\n",
        "-   Geometric interpretation: apply $Q^{\\top}$ (rotation/reflection) to\n",
        "    rotate into the basis, scale by $\\Lambda$, then apply $Q$ to\n",
        "    rotate/reflect back.\n",
        "\n",
        "## Eigendecompositions and Matrix Powers\n",
        "\n",
        "-   Can be used to find $A^t$ for large $t$ (e.g. for Markov chains)\n",
        "    -   $P^t$, i.e. $P \\cdot P \\cdot \\ldots \\cdot P$ for $t$ times\n",
        "    -   $P = Q \\Lambda Q^{-1}$ then $P^t = Q \\Lambda^t Q^{-1}$ where\n",
        "        $\\Lambda^t$ is just the pointwise power\n",
        "-   Related tools such as SVD can help with dimensionality reduction\n",
        "\n",
        "## Spectral/Eigendecomposition of Symmetric Matrix Example"
      ],
      "id": "4146bdfe-94d1-4afc-abb6-8d56f64e0db1"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eigenvectors are column-by-column in Q =\n",
            "[[-0.85065081 -0.52573111]\n",
            " [ 0.52573111 -0.85065081]]\n",
            "Q^T Q =\n",
            "[[1.0000000e+00 1.2127222e-17]\n",
            " [1.2127222e-17 1.0000000e+00]]\n",
            "Q^-1 == Q^T? True\n",
            "eigenvalues are in Lambda = [1.38196601+0.j 3.61803399+0.j]\n",
            "Q Lambda Q^T =\n",
            "[[2. 1.]\n",
            " [1. 3.]]"
          ]
        }
      ],
      "source": [
        "A = np.array([[2, 1],\n",
        "              [1, 3]])\n",
        "Lambda, Q = eig(A)\n",
        "print(f\"eigenvectors are column-by-column in Q =\\n{Q}\")\n",
        "print(f\"Q^T Q =\\n{Q.T @ Q}\") # should be I\n",
        "print(f\"Q^-1 == Q^T? {np.allclose(np.linalg.inv(Q), Q.T)}\") # should be True\n",
        "print(f\"eigenvalues are in Lambda = {Lambda}\")\n",
        "print(f\"Q Lambda Q^T =\\n{Q @ np.diag(np.real(Lambda)) @ Q.T}\")"
      ],
      "id": "3ac85064"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spectral/Eigendecomposition of an Asymmetric Matrix Example"
      ],
      "id": "da0fc883-7f7c-4751-bd03-0c5d24bf2008"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eigenvectors are column-by-column in Q =\n",
            "[[ 0.80689822 -0.34372377]\n",
            " [ 0.59069049  0.9390708 ]]\n",
            "Q^-1 == Q^T? False\n",
            "eigenvalues are in Lambda = [5.73205081+0.j 2.26794919+0.j]\n",
            "Q Lambda inv(Q) =\n",
            "[[5. 1.]\n",
            " [2. 3.]]"
          ]
        }
      ],
      "source": [
        "A = np.array([[5, 1],\n",
        "              [2, 3]])\n",
        "Lambda, Q = eig(A)\n",
        "print(f\"eigenvectors are column-by-column in Q =\\n{Q}\")\n",
        "print(f\"Q^-1 == Q^T? {np.allclose(np.linalg.inv(Q), Q.T)}\") # should be False\n",
        "print(f\"eigenvalues are in Lambda = {Lambda}\")\n",
        "print(f\"Q Lambda inv(Q) =\\n{Q @ np.diag(np.real(Lambda)) @ np.linalg.inv(Q)}\")"
      ],
      "id": "2b34cf93"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spectral Radius is Maximum Absolute Eigenvalue\n",
        "\n",
        "-   If any $\\lambda \\in \\Lambda$ are $> 1$ can see this would explode\n",
        "-   Useful for seeing if iteration $x_{t+1} = A x_t$ from a $x_0$\n",
        "    explodes\n",
        "-   The **spectral radius** of matrix $A$ is\n",
        "\n",
        "$$\n",
        "  \\rho(A) = \\max_{\\lambda \\in \\Lambda}|\\lambda|\n",
        "  $$\n",
        "\n",
        "## Recall Condition Numbers\n",
        "\n",
        "-   The condition number of the matrix $A$ is the ratio of the largest\n",
        "    and smallest eigenvalues\n",
        "    -   $\\kappa(A) = \\left|\\frac{\\lambda_{\\text{max}}}{\\lambda_{\\text{min}}}\\right|$\n",
        "        for $\\lambda$ the eigenvalues of $A$.\n",
        "    -   Sense of relative dispersion and consistent scaling\n",
        "-   For example, using the fact that the triangular matrix has\n",
        "    eigenvalues on the diagonal $$\n",
        "    A = \\begin{bmatrix}\n",
        "    1 & 2 \\\\\n",
        "    0 & 0.001\n",
        "    \\end{bmatrix}\n",
        "    $$\n",
        "    -   $\\kappa(A) = 1/0.001 = 1000$ and $\\rho(A) = 1$.\n",
        "-   Or, the best we can do is $\\kappa(A) = 1$ for the identity (or\n",
        "    triangular with 1’s on diagonal) $$\n",
        "    A = \\begin{bmatrix}\n",
        "    1 & 0 \\\\\n",
        "    0 & 1\n",
        "    \\end{bmatrix}\n",
        "    $$"
      ],
      "id": "f6d5e547-e23c-4399-8a39-eb5aeb0e8ac4"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/home/runner/work/grad_econ_datascience/grad_econ_datascience/.venv/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  }
}