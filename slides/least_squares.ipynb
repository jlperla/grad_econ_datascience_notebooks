{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Least Squares, Uniqueness, and Intro to Regularization\n",
        "\n",
        "Graduate Quantitative Economics and Datascience\n",
        "\n",
        "Jesse Perla (University of British Columbia)\n",
        "\n",
        "# Overview\n",
        "\n",
        "## Motivation\n",
        "\n",
        "-   In this section we will use some of the previous tools and discuss\n",
        "    concepts on the curvature of optimization problems\n",
        "-   Doing so, we will consider uniqueness in optimization problems in\n",
        "    datascience, economics, and ML\n",
        "-   Our key optimization problems to consider will be the quadratic\n",
        "    problems than come out of least squares regressions.\n",
        "    -   This will provide a foundation for understanding nonlinear\n",
        "        objectives since we can think of Hessians are locally quadratic.\n",
        "\n",
        "## Packages\n",
        "\n",
        "This section uses the following packages:"
      ],
      "id": "8f179693-30ea-4c4b-82bd-6c232c91a47f"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from numpy.linalg import cond, matrix_rank, norm\n",
        "from scipy.linalg import inv, solve, det, eig, lu, eigvals\n",
        "from scipy.linalg import solve_triangular, eigvalsh, cholesky"
      ],
      "id": "5db3926b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## First and Second Order Conditions in Optimization\n",
        "\n",
        "-   For univariate unconstrained optimization $\\min_x f(x)$\n",
        "-   The FONC was $f'(x) = 0$.\n",
        "    -   But this might not be a valid solution! Or there might be many\n",
        "-   The second order condition gives us more information and provides\n",
        "    sufficient conditions\n",
        "    -   if $f''(x) > 0$, then $x$ is a local minimum; if $f''(x) < 0$,\n",
        "        then $x$ is a local maximum.\n",
        "    -   if $f''(x) = 0$ then there may be multiple solutions (locally)\n",
        "\n",
        "## Definite, Semi-definite, and Convexity\n",
        "\n",
        "-   Recall in your math prep that for a univariate function $f(x)$, we\n",
        "    have:\n",
        "    -   $f(x)$ is **convex** if $f''(x) \\geq 0$ for all $x$ in the\n",
        "        domain.\n",
        "    -   $f(x)$ is **concave** if $f''(x) \\leq 0$ for all $x$ in the\n",
        "        domain.\n",
        "    -   $f(x)$ is **strictly convex** if $f''(x) > 0$ for all $x$ in the\n",
        "        domain.\n",
        "    -   $f(x)$ is **strictly concave** if $f''(x) < 0$ for all $x$ in\n",
        "        the domain.\n",
        "-   We will generalize these concepts for thinking about multivariate\n",
        "    functions\n",
        "\n",
        "## Reminder: Positive Definite"
      ],
      "id": "b18cb3e9-2a9d-437d-856d-1d4d9c83975d"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.38196601 3.61803399]\n",
            "pos-def? True\n",
            "pos-semi-def? True"
          ]
        }
      ],
      "source": [
        "A = np.array([[3, 1], [1, 2]])\n",
        "# A_eigs = np.real(eigvals(A)) # symmetric matrices have real eigenvalues\n",
        "A_eigs = eigvalsh(A) # specialized for symmetric/hermitian matrices\n",
        "print(A_eigs)\n",
        "is_positive_definite = np.all(A_eigs > 0)\n",
        "is_positive_semi_definite = np.all(A_eigs >= 0) # or eigvals(A) >= -eps\n",
        "print(f\"pos-def? {is_positive_definite}\")\n",
        "print(f\"pos-semi-def? {is_positive_semi_definite}\")"
      ],
      "id": "b8a57e52"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reminder: Positive Definite"
      ],
      "id": "0e5f5925-7a95-411a-b6d9-46d12a500917"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.99009805 3.00990195]\n",
            "pos-def? True\n",
            "pos-semi-def? True"
          ]
        }
      ],
      "source": [
        "A = np.array([[3, -0.5], [-0.1, 2]])\n",
        "# A_eigs = np.real(eigvals(A)) # symmetric matrices have real eigenvalues\n",
        "A_eigs = eigvalsh(A) # specialized for symmetric/hermitian matrices\n",
        "print(A_eigs)\n",
        "is_positive_definite = np.all(A_eigs > 0)\n",
        "is_positive_semi_definite = np.all(A_eigs >= 0) # or eigvals(A) >= -eps\n",
        "print(f\"pos-def? {is_positive_definite}\")\n",
        "print(f\"pos-semi-def? {is_positive_semi_definite}\")"
      ],
      "id": "3a3535f3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reminder: Positive Semi-Definite Matrices\n",
        "\n",
        "The simplest positive-semi-definite (but not posdef) matrix is"
      ],
      "id": "ddc807f1-d10a-4e9f-9918-63bda3d55cf0"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1.]\n",
            "pos-def? False\n",
            "pos-semi-def? True"
          ]
        }
      ],
      "source": [
        "A_eigs = eigvalsh(np.array([[1, 0], [0, 0]]))\n",
        "print(A_eigs)\n",
        "is_positive_definite = np.all(A_eigs > 0)\n",
        "is_positive_semi_definite = np.all(A_eigs >= 0) # or eigvals(A) >= -eps\n",
        "print(f\"pos-def? {is_positive_definite}\")\n",
        "print(f\"pos-semi-def? {is_positive_semi_definite}\")"
      ],
      "id": "0a8290c8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Negative Definite Matrices\n",
        "\n",
        "-   Simply swap the inequality. Think of a convex vs. concave function"
      ],
      "id": "33518024-2b15-45cb-b975-2b59c160cc5a"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-3.00990195 -1.99009805]\n",
            "neg-def? True, neg-semi-def? True"
          ]
        }
      ],
      "source": [
        "A = -1 * np.array([[3, -0.5], [-0.1, 2]])\n",
        "A_eigs = eigvalsh(A)\n",
        "print(A_eigs)\n",
        "is_negative_definite = np.all(A_eigs < 0)\n",
        "is_negative_semi_definite = np.all(A_eigs <= 0) # or eigvals(A) >= -eps\n",
        "print(f\"neg-def? {is_negative_definite}, neg-semi-def? {is_negative_semi_definite}\")"
      ],
      "id": "3a142233"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Negative Definite Matrix\n",
        "\n",
        "-   Semi-definite, but not definite requires the matrix to not be full\n",
        "    rank (i.e., an eigenvalue of 0)"
      ],
      "id": "23f7f69d-7171-4ea8-941f-917f8212470a"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-2.  0.]\n",
            "neg-def? False, neg-semi-def? True"
          ]
        }
      ],
      "source": [
        "A = np.array([[-1, -1], [-1, -1]])\n",
        "A_eigs = eigvalsh(A)\n",
        "print(A_eigs)\n",
        "is_negative_definite = np.all(A_eigs < 0)\n",
        "is_negative_semi_definite = np.all(A_eigs <= 0) # or eigvals(A) >= -eps\n",
        "print(f\"neg-def? {is_negative_definite}, neg-semi-def? {is_negative_semi_definite}\")"
      ],
      "id": "35d6ab34"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Least Squares and the Normal Equations\n",
        "\n",
        "## Least Squares\n",
        "\n",
        "Given a matrix $X \\in \\mathbb{R}^{N \\times M}$ and a vector\n",
        "$y \\in \\mathbb{R}^N$, we want to find $\\beta \\in \\mathbb{R}^M$ such that\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\min_{\\beta} &||y - X \\beta||^2, \\text{ that is,}\\\\\n",
        "\\min_{\\beta} &\\sum_{n=1}^N \\frac{1}{N}(y_n - X_n \\cdot \\beta)^2\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Where $X_n$ is n’th row. Take FOCS and rearrange to get\n",
        "\n",
        "$$\n",
        "(X^T X) \\beta =  X^T y\n",
        "$$\n",
        "\n",
        "## Solving the Normal Equations\n",
        "\n",
        "-   The $X$ is often referred to as the “design matrix”. $X^T X$ as the\n",
        "    Gram matrix\n",
        "\n",
        "-   Can form $A = X^T X$ and $b = X^T y$ and solve $A \\beta = b$.\n",
        "\n",
        "    -   Or invert $X^T X$ to get\n",
        "\n",
        "    $$\n",
        "    \\beta = (X^T X)^{-1} X^T y\n",
        "    $$\n",
        "\n",
        "    -   Note that $X^T X$ is symmetric and, if $X$ is full-rank,\n",
        "        positive definite\n",
        "\n",
        "## Solving Regression Models in Practice\n",
        "\n",
        "-   In practice, use the `lstsq` function in scipy\n",
        "    -   It uses better algorithms using eigenvectors. More stable (see\n",
        "        next lecture on conditioning)\n",
        "    -   One algorithm uses another factoring, the QR decomposition\n",
        "    -   There, $X = Q R$ for $Q$ orthogonal and $R$ upper triangular.\n",
        "        See [QR\n",
        "        Decomposition](https://python.quantecon.org/qr_decomp.html) for\n",
        "        more\n",
        "-   Better yet, for applied work use higher-level libraries like\n",
        "    `statsmodels` (integrates well with `pandas` and `seaborn`)\n",
        "    -   See [statsmodels\n",
        "        docs](https://www.statsmodels.org/dev/example_formulas.html) for\n",
        "        R-style notation\n",
        "    -   See [QuantEcon OLS Notes](https://python.quantecon.org/ols.html)\n",
        "        for more.\n",
        "\n",
        "## Example of LLS using Scipy"
      ],
      "id": "e86af654-3a3d-4194-9b81-544102edba7c"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beta =\n",
            " [ 0.96527585 -0.64393442  0.70843195  0.02742802 -1.44379448]\n",
            "beta_hat =\n",
            "[ 0.96139586 -0.63943654  0.71766934  0.03237551 -1.4455064 ]"
          ]
        }
      ],
      "source": [
        "N, M = 100, 5\n",
        "X = np.random.randn(N, M)\n",
        "beta = np.random.randn(M)\n",
        "y = X @ beta + 0.05 * np.random.randn(N)\n",
        "beta_hat, residuals, rank, s = scipy.linalg.lstsq(X, y)\n",
        "print(f\"beta =\\n {beta}\\nbeta_hat =\\n{beta_hat}\")"
      ],
      "id": "eee5b9cd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solving using the Normal Equations\n",
        "\n",
        "Or we can solve it directly. Provide matrix structure (so it can use a\n",
        "Cholesky)"
      ],
      "id": "f67ea4d3-cf56-4f84-870f-28547a7c9689"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beta =\n",
            " [ 0.96527585 -0.64393442  0.70843195  0.02742802 -1.44379448]\n",
            "beta_hat =\n",
            "[ 0.96139586 -0.63943654  0.71766934  0.03237551 -1.4455064 ]"
          ]
        }
      ],
      "source": [
        "beta_hat = solve(X.T @ X, X.T @ y, assume_a=\"pos\")\n",
        "print(f\"beta =\\n {beta}\\nbeta_hat =\\n{beta_hat}\")"
      ],
      "id": "42391266"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collinearity in “Tall” Matrices\n",
        "\n",
        "-   Tall $\\mathbb{R}^{N\\times M}$ “design matrices” have $N > M$ and are\n",
        "    “overdetermined”\n",
        "-   The rank of a matrix is full rank if all columns are linearly\n",
        "    independent\n",
        "-   You can only identify $M$ parameters with $M$ linearly independent\n",
        "    columns"
      ],
      "id": "bf419c43-cf3c-4c2b-9ef0-416297fc461f"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rank(X) = 2, rank(X_col) = 1"
          ]
        }
      ],
      "source": [
        "X = np.array([[1, 2], [2, 5], [3, 7]]) # 3 observations, 2 variables\n",
        "X_col = np.array([[1, 2], [2, 4], [3, 6]]) # all proportional\n",
        "print(f\"rank(X) = {matrix_rank(X)}, rank(X_col) = {matrix_rank(X_col)}\")"
      ],
      "id": "f941cb4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collinearity and Estimation\n",
        "\n",
        "-   If $X$ is not full rank, then $X^T X$ is not invertible. For\n",
        "    example:"
      ],
      "id": "c5a9fc00-86cf-4aa9-8426-2258c3315223"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cond(X'*X)=2819.332978639814, cond(X_col'*X_col)=1.1014450683078442e+16"
          ]
        }
      ],
      "source": [
        "print(f\"cond(X'*X)={cond(X.T@X)}, cond(X_col'*X_col)={cond(X_col.T@X_col)}\")"
      ],
      "id": "ad11854a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Note that when you start doing operations on matrices, numerical\n",
        "    error creeps in, so you will not get an exact number\n",
        "-   The rule-of-thumb with condition numbers is that if it is\n",
        "    $1\\times 10^k$ then you lose about $k$ digits of precision. So this\n",
        "    effectively means it is singular\n",
        "-   Given the singular matrix, this means a continuum of $\\beta$ will\n",
        "    solve the problem\n",
        "\n",
        "## `lstsq` Solves it? Careful on Interpretation!\n",
        "\n",
        "-   Since $X_{col}^T X_{col}$ is singular, we cannot use\n",
        "    `solve(X.T@X, y)`\n",
        "-   But what about `lstsq` methods?\n",
        "-   As you will see, this gives an answer. Interpretation is hard\n",
        "-   The key is that in the case of non-full rank, you cannot identify\n",
        "    individual parameters\n",
        "    -   Related to “Identification” in econometrics\n",
        "    -   Having low residuals is not enough"
      ],
      "id": "333098d0-6df6-47b3-9a8d-e3e24eb7345d"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beta_hat_col = [0.99857143 1.99714286]\n",
            "rank=1, cols=2, norm(X*beta_hat_col-y)=0.0"
          ]
        }
      ],
      "source": [
        "y = np.array([5.0, 10.1, 14.9])\n",
        "beta_hat, residuals, rank, s = scipy.linalg.lstsq(X_col, y)\n",
        "print(f\"beta_hat_col = {beta_hat}\")\n",
        "print(f\"rank={rank}, cols={X.shape[1]}, norm(X*beta_hat_col-y)={norm(residuals)}\")"
      ],
      "id": "49898db6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fat Design Matrices\n",
        "\n",
        "-   Fat $\\mathbb{R}^{N\\times M}$ “design matrices” have $N < M$ and are\n",
        "    “underdetermined”\n",
        "-   Less common in econometrics, but useful to understand the structure\n",
        "-   A continuum $\\beta\\in\\mathbb{R}^{M - \\text{rank}(X)}$ solve this\n",
        "    problem"
      ],
      "id": "20d63c48-f169-45f3-840f-5bae6a0d09e4"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beta_hat = [0.8 0.6 1. ], rank=2, ? residuals = []"
          ]
        }
      ],
      "source": [
        "X = np.array([[1, 2, 3], [0, 5, 7]]) # 2 rows, 3 variables\n",
        "y = np.array([5, 10])\n",
        "beta_hat, residuals, rank, s = scipy.linalg.lstsq(X, y)\n",
        "print(f\"beta_hat = {beta_hat}, rank={rank}, ? residuals = {residuals}\")"
      ],
      "id": "08d66b41"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Which Solution?\n",
        "\n",
        "-   Residuals are zero here because there are enough parameters to fit\n",
        "    perfectly (i.e., it is underdetermined)\n",
        "-   Given the multiple solutions, the `lstsq` is giving\n",
        "\n",
        "$$\n",
        "\\min_{\\beta} ||\\beta||_2^2 \\text{ s.t. } X \\beta = y\n",
        "$$\n",
        "\n",
        "-   i.e., the “smallest” coefficients which interpolate the data exactly\n",
        "-   Which trivially fulfills the OLS objective:\n",
        "    $\\min_{\\beta} ||y - X \\beta||_2^2$\n",
        "\n",
        "## Careful Interpreting Underdetermined Solutions\n",
        "\n",
        "-   Useful and common in ML, but be **very** careful when interpreting\n",
        "    for economics\n",
        "    -   Tight connections to Bayesian versions of statistical tests\n",
        "    -   But until you understand econometrics and “identification” well,\n",
        "        **stick to full-rank matrices**\n",
        "    -   **Advanced topics:** search for “Regularization”, “Ridgeless\n",
        "        Regression” and “Benign Overfitting in Linear Regression.”"
      ],
      "id": "add6b9bb-d00a-4267-bb74-1e0e6920c540"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/home/runner/work/grad_econ_datascience/grad_econ_datascience/.venv/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  }
}